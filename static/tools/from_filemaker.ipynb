{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing InKind from FileMaker\n",
    "\n",
    "We use an XML export of the various tables in the FileMaker Inkind database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os,sys,re,collections\n",
    "from os.path import splitext, basename\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "from glob import glob\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table and Field organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DB_NAME = 'inkind_data'\n",
    "\n",
    "MERGE_FIELDS = dict(\n",
    "    contrib=dict(\n",
    "        academic_entity_url=('academic_entity_url_2',),\n",
    "        contribution_url=('contribution_url_2',),\n",
    "        contact_person_mail=('contact_person_mail_2',),\n",
    "    ),\n",
    ")\n",
    "\n",
    "SKIP_FIELDS = dict(\n",
    "    contrib={\n",
    "        'teller',\n",
    "        'whois',\n",
    "        'help_text',\n",
    "        'help_description',\n",
    "        'total_costs_total',\n",
    "        'goldpassword',\n",
    "        'gnewpassword',\n",
    "        'gnewpassword2',\n",
    "    },\n",
    ")\n",
    "\n",
    "MOVE_FIELDS = dict(\n",
    "    contrib=dict(\n",
    "        assessment={\n",
    "            'submit',\n",
    "            'approved',\n",
    "            'vcchead_approval',\n",
    "            'vcchead_disapproval',\n",
    "            'dateandtime_approval',\n",
    "            'dateandtime_cioapproval',\n",
    "            'dateandtime_ciozero',\n",
    "            'vcc_head_decision',\n",
    "            'vcc_head_decision_vcc11',\n",
    "            'vcc_head_decision_vcc12',\n",
    "            'vcc_head_decision_vcc21',\n",
    "            'vcc_head_decision_vcc22',\n",
    "            'vcc_head_decision_vcc31',\n",
    "            'vcc_head_decision_vcc32',\n",
    "            'vcc_head_decision_vcc41',\n",
    "            'vcc_head_decision_vcc42',\n",
    "            'vcc11_name',\n",
    "            'vcc12_name',\n",
    "            'vcc21_name',\n",
    "            'vcc22_name',\n",
    "            'vcc31_name',\n",
    "            'vcc32_name',\n",
    "            'vcc41_name',\n",
    "            'vcc42_name',\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "FIELD_TYPE_OVERRIDE = dict(\n",
    "    contrib=dict(\n",
    "        costs_total='valuta',\n",
    "        total_costs_total='valuta',\n",
    "        creation_date_time='datetime',\n",
    "        modification_date_time='datetime',\n",
    "    ),\n",
    "    assessment=dict(\n",
    "        dateandtime_approval='datetime',\n",
    "        dateandtime_cioapproval='datetime',\n",
    "        dateandtime_ciozero='datetime',\n",
    "    ),\n",
    ")\n",
    "\n",
    "SPLIT_FIELDS = dict(\n",
    "    contrib=dict(\n",
    "        disciplines_associated='generic',\n",
    "        other_keywords='generic_comma',\n",
    "        tadirah_research_activities='generic',\n",
    "        tadirah_research_objects='generic',\n",
    "        tadirah_research_techniques='generic',\n",
    "        type_of_inkind='generic',\n",
    "        vcc='generic',\n",
    "    ),\n",
    ")\n",
    "\n",
    "VALUE_FIELDS = dict(\n",
    "    contrib={\n",
    "        'country',\n",
    "        'creator',\n",
    "        'last_modifier',\n",
    "        'other_type_of_inkind',\n",
    "        'year',\n",
    "    },\n",
    ")\n",
    "\n",
    "FIX_FIELDS = dict(\n",
    "    contrib=dict(\n",
    "        country='countrycode',\n",
    "    ),\n",
    ")\n",
    "\n",
    "LIMIT_ROWS = 50 # maximum number of rows to be written in one sql insert statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TYPES = {'number', 'text', 'valuta', 'date', 'datetime'}\n",
    "\n",
    "MIN_M = 5       # minimum varchar size = 2**MIN_M\n",
    "MAX_M = 13      # maximum varchar size = 2**MAX_M\n",
    "\n",
    "DATE_PATTERN = re.compile(\n",
    "    '^\\s*([0-9]{2})/([0-9]{2})/([0-9]{4})$'\n",
    ")\n",
    "DATE2_PATTERN = re.compile(\n",
    "    '^\\s*([0-9]{4})-([0-9]{2})-([0-9]{2})$'\n",
    ")\n",
    "DATETIME_PATTERN = re.compile(\n",
    "    '^\\s*([0-9]{2})/([0-9]{2})/([0-9]{4})\\s+([0-9]{2}):([0-9]{2})(?::([0-9]{2}))?$'\n",
    ")\n",
    "NULL_VALUES = {\n",
    "    'http://',\n",
    "    'https://',\n",
    "}\n",
    "\n",
    "TBF = '_tobefixed'\n",
    "\n",
    "splitters = dict(\n",
    "    generic=re.compile('[ \\t]*[\\n+][ \\t\\n]*'),\n",
    "    generic_comma=re.compile('[ \\t]*[\\n+,][ \\t\\n]*'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Locations\n",
    "\n",
    "HOME_DIR = os.path.expanduser('~').replace('\\\\', '/')\n",
    "BASE_DIR = '{}/projects/has/dacs'.format(HOME_DIR)\n",
    "FM_DIR = '{}/fm'.format(BASE_DIR)\n",
    "TEMP_DIR = '{}/tmp'.format(BASE_DIR)\n",
    "RESULT_DIR = '{}/sql'.format(BASE_DIR)\n",
    "FMNS = '{http://www.filemaker.com/fmpxmlresult}'\n",
    "ROW_RAW_FILE = '{}/row_raw_file'.format(TEMP_DIR)\n",
    "ROW_FILE = '{}/row_file'.format(TEMP_DIR)\n",
    "ROW_EXT = 'txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nwarnings = 0\n",
    "\n",
    "def resetw():\n",
    "    global nwarnings\n",
    "    nwarnings = 0\n",
    "\n",
    "def info(msg):\n",
    "    sys.stdout.write('{}\\n'.format(msg))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def note(msg):\n",
    "    sys.stdout.write('NB: {}\\n'.format(msg))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def warning(msg, count=True):\n",
    "    global nwarnings\n",
    "    sys.stderr.write('{} {}: {}\\n'.format('!'*5, 'WARNING', msg))\n",
    "    sys.stderr.flush()\n",
    "    if count: nwarnings += 1\n",
    "\n",
    "def finalw():\n",
    "    if nwarnings == 0:\n",
    "        info('OK, no warnings')\n",
    "    else:\n",
    "        warning('There were {} warnings'.format(nwarnings), count=False)\n",
    "\n",
    "def check_config():\n",
    "    good = True\n",
    "    for x in [1]:\n",
    "        good = False\n",
    "        if not os.path.exists(BASE_DIR):\n",
    "            warning('BASE_DIR does not exist: {}'.format(BASE_DIR))\n",
    "            break\n",
    "        this_good = True\n",
    "        for cdir in (TEMP_DIR, RESULT_DIR):\n",
    "            this_good = False\n",
    "            if not os.path.exists(cdir):\n",
    "                try:\n",
    "                    os.makedirs(cdir)\n",
    "                except os.error as e:\n",
    "                    warning('{} could not be created.'.format(cdir))\n",
    "                    break\n",
    "            this_good = True\n",
    "        if not this_good:\n",
    "            break\n",
    "        good = True\n",
    "    if not good:\n",
    "        warning('There were configuration errors', count=False)\n",
    "    else:\n",
    "        info('Configuration OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value validation and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def date_repl(match):\n",
    "    [d,m,y] = list(match.groups())\n",
    "    return '{}-{}-{}'.format(y,m,d)\n",
    "    \n",
    "def date2_repl(match):\n",
    "    [y,m,d] = list(match.groups())\n",
    "    return '{}-{}-{}'.format(y,m,d)\n",
    "    \n",
    "def datetime_repl(match):\n",
    "    [d,m,y,hr,mn,sc] = list(match.groups())\n",
    "    return '{}-{}-{}T{}:{}:{}'.format(y,m,d,hr,mn,sc or '00')\n",
    "    \n",
    "def sq(v_raw):\n",
    "    return \"'{}'\".format(\n",
    "        v_raw.strip().replace(\"'\",\"''\").replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "    )\n",
    "\n",
    "def num(v_raw, i, t, fname):\n",
    "    if v_raw.isdigit(): return int(v_raw)\n",
    "    warning(\n",
    "        'table `{}` field `{}` record {}: not an integer: \"{}\"'.format(\n",
    "            t, fname, i, v_raw\n",
    "    ))\n",
    "    return v_raw\n",
    "\n",
    "money_warnings = {}\n",
    "money_notes = {}\n",
    "\n",
    "def money(v_raw, i, t, fname):\n",
    "    note = ',' in v_raw or '.' in v_raw\n",
    "    v = v_raw.strip().replace(' ','').replace('â‚¬', '').replace('\\u00a0', '')\n",
    "    for p in range(2,4): # interpret . or , as decimal point if less than 3 digits follow it\n",
    "        if len(v) >= p and v[-p] in '.,': \n",
    "            v_i = v[::-1]\n",
    "            if v_i[p-1] == ',': v_i = v_i.replace(',', 'D', 1)\n",
    "            elif v_i[p-1] == '.': v_i = v_i.replace('.', 'D', 1)\n",
    "            v = v_i[::-1]\n",
    "    v = v.replace('.','').replace(',','')\n",
    "    v = v.replace('D', '.')\n",
    "    if not v.replace('.','').isdigit():\n",
    "        warning(\n",
    "            'table `{}` field `{}` record {}: not a decimal number: \"{}\" <= \"{}\"'.format(\n",
    "                t, fname, i, v, v_raw,\n",
    "        ))\n",
    "        money_warnings.setdefault('{}:{}'.format(t, fname), {}).setdefault(v, set()).add(v_raw)\n",
    "    elif note:\n",
    "        money_notes.setdefault('{}:{}'.format(t, fname), {}).setdefault(v, set()).add(v_raw)\n",
    "    return v\n",
    "\n",
    "def dt(v_raw, i, t, fname):\n",
    "    if not DATE2_PATTERN.match(v_raw):\n",
    "        warning(\n",
    "            'table `{}` field `{}` record {}: not a valid date: \"{}\"'.format(\n",
    "                t, fname, i, v_raw\n",
    "        ))\n",
    "        return v_raw\n",
    "    return(\"'{}'\".format(DATE2_PATTERN.sub(date2_repl, v_raw)))\n",
    "\n",
    "def dtm(v_raw, i, t, fname):\n",
    "    if not DATETIME_PATTERN.match(v_raw):\n",
    "        warning(\n",
    "            'table `{}` field `{}` record {}: not a valid date time: \"{}\"'.format(\n",
    "                t, fname, i, v_raw\n",
    "        ))\n",
    "        return v_raw\n",
    "    return(\"'{}'\".format(DATETIME_PATTERN.sub(datetime_repl, v_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_fm():\n",
    "    main_tables_raw = []\n",
    "    parser = etree.XMLParser(remove_blank_text=True, ns_clean=True)\n",
    "    root = {}\n",
    "    for infile in glob('{}/*.xml'.format(FM_DIR)):\n",
    "        tname = basename(splitext(infile)[0])\n",
    "        print('Parsing {}'.format(tname))\n",
    "        root[tname] = etree.parse(infile, parser).getroot()\n",
    "        main_tables_raw.append(tname)\n",
    "    return (root, main_tables_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process field specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_merge():\n",
    "    merge_errors = 0\n",
    "    merge_fields = {}\n",
    "\n",
    "    for t in main_tables_raw:\n",
    "        for (mfhead, mftail) in MERGE_FIELDS.get(t, {}).items():\n",
    "            for f in mftail:\n",
    "                if f in merge_fields.get(t, {}):\n",
    "                    warning(\n",
    "    'table `{}` field `{}` already merged into `{}` now to be merged into `{}`'.format(\n",
    "                        t, f, merge_fields[t][f], mfhead,\n",
    "                    ))\n",
    "                    merge_errors += 1\n",
    "                merge_fields.setdefault(t, {})[f] = mfhead\n",
    "    if merge_errors:\n",
    "        warning('There were {} merge errors'.format(merge_errors), count=False)\n",
    "    else:\n",
    "        info('Merge definitions OK')\n",
    "    return merge_fields\n",
    "\n",
    "def getfielddefs():\n",
    "    field_defs_raw = {}\n",
    "    fd_errors = 0\n",
    "    tfields = {}\n",
    "    for t in main_tables_raw:\n",
    "        fieldroots = [x for x in root[t].iter(FMNS+'METADATA')]\n",
    "        fieldroot = fieldroots[0]\n",
    "        tfields[t] = []\n",
    "        for x in fieldroot.iter(FMNS+'FIELD'):\n",
    "            fname = x.get('NAME').lower().replace(' ','_').replace(':', '_')\n",
    "            ftype = FIELD_TYPE_OVERRIDE.\\\n",
    "                get(t, {}).\\\n",
    "                get(fname, None) or x.get('TYPE').lower()\n",
    "            fmult = int(x.get('MAXREPEAT'))\n",
    "            if fname in SPLIT_FIELDS.get(t, {}): fmult += 1\n",
    "            tfields[t].append(fname)\n",
    "            field_defs_raw.setdefault(t, {})[fname] = [ftype, fmult]\n",
    "            if ftype not in TYPES:\n",
    "                warning('table `{}` field `{}` has unknown type \"{}\"'.format(\n",
    "                    t, fname, ftype,\n",
    "                ))\n",
    "                fd_errors += 1\n",
    "        info('Table {:<20}: {:>2} fields'.format(t, len(tfields[t])))\n",
    "    if fd_errors:\n",
    "        warning('There were {} field definition errors'.format(fd_errors), count=False)\n",
    "    else:\n",
    "        info('Field definitions OK')\n",
    "    return (tfields, field_defs_raw)\n",
    "\n",
    "def check_merge_more():\n",
    "    merge_errors = 0\n",
    "    for t in main_tables_raw:\n",
    "        for f in merge_fields.get(t, {}):\n",
    "            if f not in field_defs_raw[t]:\n",
    "                warning(\n",
    "                    'table `{}`: cannot merge unknown field `{}`'.format(\n",
    "                    t, f,\n",
    "                ))\n",
    "                merge_errors += 1\n",
    "                continue\n",
    "            ftarget = merge_fields[t][f]\n",
    "            (ftype, fmult) = field_defs_raw[t][f]\n",
    "            if ftarget not in field_defs_raw[t]:\n",
    "                field_defs_raw[t][ftarget] = [ftype, 0]\n",
    "            (ttype, tmult) = field_defs_raw[t][ftarget]\n",
    "            if ttype != ftype:\n",
    "                warning(\n",
    "                    'table `{}` field `{}` of type \"{}\" is merged into field `{}` of other type \"{}\"'.format(\n",
    "                        t, f, ftype, ftarget, ttype,\n",
    "                ))\n",
    "                merge_errors += 1\n",
    "            field_defs_raw[t][ftarget][1] += fmult\n",
    "            del field_defs_raw[t][f]\n",
    "    if merge_errors:\n",
    "        warning('There were {} merge errors'.format(merge_errors), count=False)\n",
    "    else:\n",
    "        info('Merge OK')\n",
    "\n",
    "def do_skips():\n",
    "    fields_raw = {}\n",
    "    s_errors = 0\n",
    "    for t in main_tables_raw:\n",
    "        for f in SKIP_FIELDS.get(t, set()):\n",
    "            if f not in field_defs_raw[t]:\n",
    "                warning('table `{}`: unknown skip field `{}`'.format(t,f))\n",
    "                s_errors += 1\n",
    "            else:\n",
    "                del field_defs_raw[t][f]\n",
    "        fields_raw[t] = sorted(\n",
    "            set(field_defs_raw[t].keys()) | set(merge_fields.get(t, {}).values())\n",
    "        )\n",
    "    if s_errors:\n",
    "        warning('There were {} field skip errors'.format(s_errors), count=False)\n",
    "    else:\n",
    "        info('Field skips OK')\n",
    "\n",
    "    return fields_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getdata():\n",
    "    rows_raw = {}\n",
    "    errors = {}\n",
    "\n",
    "    for t in main_tables_raw:\n",
    "        dataroots = [x for x in root[t].iter(FMNS+'RESULTSET')]\n",
    "        dataroot = dataroots[0]\n",
    "        rows_raw[t] = []\n",
    "\n",
    "        for (i, r) in enumerate(dataroot.iter(FMNS+'ROW')):\n",
    "            row = []\n",
    "            for c in r.iter(FMNS+'COL'):\n",
    "                data = [x.text for x in c.iter(FMNS+'DATA')]\n",
    "                row.append(data)\n",
    "            if len(row) != len(tfields[t]):\n",
    "                errors.setdefault(t, {}).setdefault('Number of fields', []).append(i)\n",
    "            rows_raw[t].append(row)\n",
    "\n",
    "        rf = open('{}_{}.{}'.format(ROW_RAW_FILE, t, ROW_EXT), 'w')\n",
    "        for row in rows_raw[t]:\n",
    "            for (fname, values) in zip(tfields[t], row):\n",
    "                rf.write('@{:>30} = {}\\n'.format(\n",
    "                    fname,\n",
    "                    ' | '.join('{}'.format(v) for v in values),\n",
    "                ))\n",
    "            rf.write('{}\\n'.format('='*100))\n",
    "        rf.close()\n",
    "        info('Table {:<20}: {:>4} rows read'.format(t, len(rows_raw[t])))\n",
    "    if errors:\n",
    "        for t in sorted(errors):\n",
    "            for k in sorted(errors[t]):\n",
    "                warning('table {:<20}: {:<20}: {}'.format(t, k, ','.join(errors[k])))\n",
    "    else:\n",
    "        info('Data import OK')\n",
    "    return rows_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the values\n",
    "\n",
    "Various non-informational values will be converted to NULL.\n",
    "Values will be thinned: \n",
    "Identical values will be reduced to one copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transformrows():\n",
    "    rows = {}\n",
    "    money_warnings.clear()\n",
    "    for t in main_tables_raw:\n",
    "        for (i, row_raw) in enumerate(rows_raw.get(t, [])):\n",
    "            values = {}\n",
    "            for (fname, values_raw) in zip(tfields[t], row_raw):\n",
    "                if fname in SKIP_FIELDS.get(t, set()): continue\n",
    "                sep = SPLIT_FIELDS.get(t, {}).get(fname, None)\n",
    "                if sep != None:\n",
    "                    values_raw = sorted(reduce(\n",
    "                        set.union, \n",
    "                        [set(splitters[sep].split(v)) for v in values_raw if v != None], \n",
    "                        set(),\n",
    "                    ))\n",
    "                    if '' in values_raw: values_raw.remove('')\n",
    "                ftarget = merge_fields.get(t, {}).get(fname, fname)\n",
    "                (ftype, fmult) = field_defs_raw[t][ftarget]\n",
    "                valset = set()\n",
    "                for v_raw in values_raw:\n",
    "                    if v_raw == None or v_raw in NULL_VALUES: v = 'NULL'\n",
    "                    elif ftype == 'text': v = sq(v_raw)\n",
    "                    elif ftype == 'number': v = num(v_raw, i, t, fname)\n",
    "                    elif ftype == 'valuta': v = money(v_raw, i, t, fname)\n",
    "                    elif ftype == 'date': v = dt(v_raw, i, t, fname)\n",
    "                    elif ftype == 'datetime': v = dtm(v_raw, i, t, fname)\n",
    "                    else: v = v_raw\n",
    "                    valset.add(v)\n",
    "                if fmult > 1: valset.discard('NULL')\n",
    "                these_values = values.setdefault(ftarget, set())\n",
    "                these_values |= valset\n",
    "            rows.setdefault(t, []).append(values)\n",
    "        info('Table `{}`: {:>5} rows checked'.format(t, len(rows[t])))\n",
    "\n",
    "        rf = open('{}_{}.{}'.format(ROW_FILE, t, ROW_EXT), 'w')\n",
    "        for row in rows[t]:\n",
    "            for (fname, values) in sorted(row.items()):\n",
    "                rf.write('@{:>30} = {}\\n'.format(\n",
    "                    fname,\n",
    "                    ' | '.join('{}'.format(v) for v in sorted(values)),\n",
    "                ))\n",
    "            rf.write('{}\\n'.format('='*100))\n",
    "        rf.close()\n",
    "\n",
    "    if money_notes:\n",
    "        for tf in sorted(money_notes):\n",
    "            for v in sorted(money_notes[tf]):\n",
    "                note('{} \"{}\" <= {}'.format(\n",
    "                    tf, v,\n",
    "                    ' | '.join(money_notes[tf][v]),\n",
    "            ))\n",
    "\n",
    "    if money_warnings:\n",
    "        for tf in sorted(money_warnings):\n",
    "            for v in sorted(money_warnings[tf]):\n",
    "                warning('{} \"{}\" <= {}'.format(\n",
    "                    tf, v,\n",
    "                    ' | '.join(money_warnings[tf][v]),\n",
    "            ))\n",
    "    else:\n",
    "        info('Money OK')\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn the data into a dict\n",
    "\n",
    "We represent the data with a dictionary. The keys are the field names.\n",
    "The values are dictionaries again, with keys new ids and with values the value that the row with that id has for that field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pivot():\n",
    "    field_data_raw = {}\n",
    "    for t in main_tables_raw:\n",
    "        for row in rows[t]:\n",
    "            for (fname, values) in sorted(row.items()):\n",
    "                field_data_raw.setdefault(t, {}).setdefault(fname, []).append(values)\n",
    "        info('Table `{}`: {:<5} records and {:<2} fields pivoted'.format(\n",
    "            t, len(rows[t]), len(field_data_raw[t]),\n",
    "        ))\n",
    "\n",
    "    # check\n",
    "    good = True\n",
    "    for t in field_data_raw:\n",
    "        for f in field_data_raw[t]:\n",
    "            if len(field_data_raw[t][f]) != len(rows[t]):        \n",
    "                warning(\n",
    "        'table `{}`, field `{}`: wrong number of records: {} instead of {}'.format(\n",
    "                    t, f, len(field_data_raw[t][f]), len(rows[t]),\n",
    "                ))\n",
    "                good = False\n",
    "    if good:\n",
    "        info('Pivot OK')\n",
    "    else:\n",
    "        warning('There were errors', count=False)\n",
    "    return field_data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move_fields():\n",
    "    errors = 0\n",
    "    main_tables = deepcopy(main_tables_raw)\n",
    "    fields = deepcopy(fields_raw)\n",
    "    field_defs = deepcopy(field_defs_raw)\n",
    "    field_data = deepcopy(field_data_raw)\n",
    "    for t in MOVE_FIELDS:\n",
    "        if t not in field_data:\n",
    "            warning('move fields from table `{}`: this table does not exist'.format(\n",
    "                t,\n",
    "            ))\n",
    "            errors += 1\n",
    "            continue\n",
    "        for t_new in MOVE_FIELDS[t]:\n",
    "            main_tables.append(t_new)\n",
    "            nid = '{}_id'.format(t)\n",
    "            field_data.setdefault(t_new, {})[nid] = [{i} for i in range(len(rows[t]))]\n",
    "            field_defs.setdefault(t_new, {})[nid] = ((t, 'id'), 1)\n",
    "            move_fields = set(MOVE_FIELDS[t][t_new])\n",
    "            for f in sorted(move_fields):\n",
    "                if f not in field_data[t]:\n",
    "                    warning(\n",
    "            'table `{}`: move field `{}` to `{}`: this field does not exist'.format(\n",
    "                        t, f, t_new,\n",
    "                    ))\n",
    "                    errors += 1\n",
    "                    move_fields.remove(f)\n",
    "                    continue\n",
    "                field_data.setdefault(t_new, {})[f] = field_data[t][f]\n",
    "                del field_data[t][f]\n",
    "                field_defs.setdefault(t_new, {})[f] = field_defs[t][f]\n",
    "                del field_defs[t][f]\n",
    "            fields[t] = sorted(set(fields[t]) - move_fields)\n",
    "            fields[t_new] = [nid]+sorted(move_fields)\n",
    "            info('moved fields\\n\\t`{}`\\nfrom `{}` to `{}`'.format(\n",
    "                '`\\n\\t`'.join(sorted(move_fields)), t, t_new, \n",
    "            ))\n",
    "            \n",
    "    if errors:\n",
    "        warning('There were {} errors'.format(errors), count=False)\n",
    "    else:\n",
    "        info('Move fields OK')\n",
    "    return (main_tables, fields, field_defs, field_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract related tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(t, fname, maindata, relvalues, relindex, reltables, relxtables, relfieldindex):\n",
    "    fname_alt = fname\n",
    "    if fname in main_tables:\n",
    "        link_field = FIX_FIELDS.get(t, {}).get(fname, None)\n",
    "        if link_field:\n",
    "            note('table `{}`: value field `{}` will be linked to `{}:{}'.format(\n",
    "                t, fname, fname, link_field,\n",
    "        ))\n",
    "        else:\n",
    "            warning('table `{}`: value field `{}` is already a table!'.format(t, fname))\n",
    "        fname_alt = '{}{}'.format(fname, TBF)\n",
    "    is_single = field_defs[t][fname][1] == 1 # single value of multiple values\n",
    "    error = False\n",
    "    if fname in relfieldindex:\n",
    "        warning(\n",
    "    'related table `{}` extracted from `{}` and earlier from `{}`'.format(\n",
    "            fname, t, relfieldindex[fname],\n",
    "        ))\n",
    "        error = True\n",
    "    relfieldindex[fname] = t\n",
    "    for (i, values) in enumerate(field_data[t][fname]):\n",
    "        for value in values:\n",
    "            vid = relvalues.setdefault(fname, {}).get(value, None)\n",
    "            if vid == None:\n",
    "                relindex[fname] += 1\n",
    "                vid = relindex[fname]\n",
    "                reltables.setdefault(fname_alt, []).append((vid, value))\n",
    "            relvalues[fname][value] = vid\n",
    "            if is_single:\n",
    "                maindata[t][fname][i] = {vid}\n",
    "            else:\n",
    "                relxtables.setdefault(fname_alt, []).append((i, vid))\n",
    "    if not is_single: del maindata[t][fname]\n",
    "    return error\n",
    "\n",
    "def transform_data():\n",
    "    maindata = deepcopy(field_data)\n",
    "    relvalues = {} # dicts\n",
    "    relindex = collections.Counter()\n",
    "    reltables = {} # lists\n",
    "    relxtables = {} # lists\n",
    "    relfieldindex = {}\n",
    "    errors = 0\n",
    "    for t in main_tables:\n",
    "        field_list =\\\n",
    "            VALUE_FIELDS.get(t, set()) |\\\n",
    "            {f for f in fields[t] if field_defs[t][f][1] > 1}\n",
    "        for fname in field_list:\n",
    "            if fname not in field_defs[t]:\n",
    "                warning('table `{}`: wrong field {}'.format(t, fname))\n",
    "                errors += 1\n",
    "                continue\n",
    "            error = extract(\n",
    "                t, fname, maindata, \n",
    "                relvalues, relindex, reltables, relxtables, relfieldindex,\n",
    "            )\n",
    "            if error: errors +=1\n",
    "    if errors:\n",
    "        warning('There were {} extraction errors'.format(errors), count=False)\n",
    "    else:\n",
    "        info('Extraction OK')\n",
    "    return (maindata, reltables, relxtables, relvalues, relfieldindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getmapping(main_t, main_f):\n",
    "    rel_t = '{}{}'.format(main_t, TBF)\n",
    "    rel_f = main_t\n",
    "    main_codes = maindata[main_t][main_f]\n",
    "    rel_codes = reltables[rel_t]\n",
    "    main_index = dict((list(c)[0],i) for (i,c) in enumerate(main_codes))\n",
    "    rel_index = dict((i,c) for (i,c) in rel_codes)\n",
    "    return dict((i, main_index[rel_index[i]]) for i in rel_index)\n",
    "\n",
    "def fix(t, main_t, main_f):\n",
    "    mapping = getmapping(main_t, main_f)\n",
    "    rel_t = '{}{}'.format(main_t, TBF)\n",
    "    if rel_t in reltables: del reltables[rel_t]\n",
    "\n",
    "    new_maindata = [{mapping[list(x)[0]]} for x in maindata[t][main_t]]\n",
    "    maindata[t][main_t] = new_maindata\n",
    "    main_tables.remove(main_t)\n",
    "    main_tables.insert(0, main_t)\n",
    "\n",
    "def fix_them():\n",
    "    for t in FIX_FIELDS:\n",
    "        for main_t in FIX_FIELDS[t]:\n",
    "            link_field = FIX_FIELDS[t][main_t]\n",
    "            note('linking `{}:{}` to table `{}` on `{}`'.format(\n",
    "                t, main_t, main_t, link_field,\n",
    "            ))\n",
    "            fix(t, main_t, link_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write sql to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getsize(source, fname):\n",
    "    values = set()\n",
    "    for vals in source: values |= set(vals)\n",
    "    maxlen = max({len(x) for x in values if x != 'NULL'}, default=0)\n",
    "    result = 0\n",
    "    for m in range(MIN_M, MAX_M+1):\n",
    "        if maxlen <= 2**m:\n",
    "            result = m\n",
    "            break\n",
    "    if maxlen > 2**MAX_M:\n",
    "        note(\n",
    "            'Field `{}`: value with length {} gets type TEXT'.format(\n",
    "                fname, maxlen, 2**MAX_M,\n",
    "        ))\n",
    "        return False\n",
    "    return 2**m\n",
    "\n",
    "def getdef(source, t, fname, newfname, warn_mult=True):\n",
    "    (ft, fmult) = field_defs[t][fname]\n",
    "    if warn_mult and fmult > 1:\n",
    "        warning(\n",
    "            'skipping field `{}` because it contains multiple values'.format(\n",
    "                fname,\n",
    "        ))\n",
    "        return None\n",
    "    if type(ft) is tuple:\n",
    "        (ftable, ffield) = ft\n",
    "        ftype = 'int'\n",
    "        fsize = '(4)'\n",
    "        fext = ',\\n\\tforeign key ({}) references {}({})'.format(fname, ftable, ffield)\n",
    "    elif ft == 'number':\n",
    "        ftype = 'int'\n",
    "        fsize = '(4)'\n",
    "        fext = ''\n",
    "    elif ft == 'text':\n",
    "        ftype = 'varchar'\n",
    "        fsize_raw = getsize(source, fname)\n",
    "        if not fsize_raw:\n",
    "            ftype = 'text'\n",
    "            fsize = ''\n",
    "        else:\n",
    "            fsize = '({})'.format(fsize_raw)\n",
    "        fext = 'character set utf8'\n",
    "    elif ft == 'valuta':\n",
    "        ftype = 'decimal'\n",
    "        fsize = '(10,2)'\n",
    "        fext = ''\n",
    "    elif ft == 'date':\n",
    "        ftype = 'datetime'\n",
    "        fsize = ''\n",
    "        fext = ''\n",
    "    elif ft == 'datetime':\n",
    "        ftype = 'datetime'\n",
    "        fsize = ''\n",
    "        fext = ''\n",
    "    else:\n",
    "        warning('skipping field `{}` because it has unknown type `{}`'.format(\n",
    "            fname, ft,\n",
    "        ))\n",
    "        return None\n",
    "    return '{} {}{} {}'.format(newfname, ftype, fsize, fext)\n",
    "\n",
    "def getrdef(fname):\n",
    "    return '''{fn}_id int(4),\n",
    "    foreign key ({fn}_id) references {fn}(id)'''.format(fn=fname)\n",
    "\n",
    "def sql_data(df, tname, flist, rows):\n",
    "    head = 'insert into {} ({}) values'.format(tname, ','.join(flist))\n",
    "    for (i, row) in enumerate(rows):\n",
    "        if i % LIMIT_ROWS == 0:\n",
    "            if i > 0: df.write(';')\n",
    "            df.write('\\n')\n",
    "            df.write('select \"table {} row {}\" as \" \";\\n'.format(tname, i))\n",
    "            df.write(head)\n",
    "            sep = ''\n",
    "        df.write('\\n{}\\t'.format(sep))\n",
    "        sep = ','\n",
    "        df.write('({})'.format(','.join(str(x) for x in row)))\n",
    "    df.write(';\\n')\n",
    "        \n",
    "def print_maintables(maindata, reltables, cf, df):\n",
    "    errors = 0\n",
    "    for t in main_tables:\n",
    "        fdefs = ['id int(4) primary key']\n",
    "        flist = sorted(maindata[t])\n",
    "        fnewlist = []\n",
    "        for fname in flist:\n",
    "            if fname in reltables or fname in FIX_FIELDS.get(t, {}):\n",
    "                fdef = getrdef(fname)\n",
    "                fnewname = '{}_id'.format(fname)\n",
    "            else:\n",
    "                fdef = getdef(field_data[t][fname], t, fname, fname)\n",
    "                if fdef == None:\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                fnewname = fname\n",
    "            fdefs.append(fdef)\n",
    "            fnewlist.append(fnewname)\n",
    "        cf.write('''\n",
    "create table {} (\n",
    "    {}\n",
    ");\n",
    "    '''.format(t, ',\\n\\t'.join(fdefs)))\n",
    "        maintable_raw = zip(*(maindata[t][f] for f in flist))\n",
    "        maintable = [\n",
    "            [i]+[sorted(vals)[0] for vals in row] for (i, row) in enumerate(maintable_raw)\n",
    "        ]\n",
    "        sql_data(df, t, ['id'] + fnewlist, maintable)\n",
    "    return errors\n",
    "\n",
    "def print_reltables(reltables, relvalues, cf, df):\n",
    "    errors = 0\n",
    "    for tname_alt in sorted(reltables):\n",
    "        tname = tname_alt\n",
    "        pos = tname_alt.rfind(TBF)\n",
    "        if pos > 0: tname = tname_alt[0:pos]\n",
    "        fdefs = ['id int(4) primary key']\n",
    "        fdef = getdef(\n",
    "            [relvalues[tname].keys()], \n",
    "            relfieldindex[tname], \n",
    "            tname, 'val', warn_mult=False,\n",
    "        )\n",
    "        if fdef == None:\n",
    "            errors += 1\n",
    "            continue            \n",
    "        fdefs.append(fdef)\n",
    "        cf.write('''\n",
    "create table {} (\n",
    "    {}\n",
    ");\n",
    "'''.format(tname_alt, ',\\n\\t'.join(fdefs)))\n",
    "        sql_data(df, tname_alt, ['id', 'val'], reltables[tname_alt])\n",
    "    return errors\n",
    "\n",
    "def print_relxtables(relxtables, cf, df):\n",
    "    errors = 0\n",
    "    for tname_alt in sorted(relxtables):\n",
    "        tname = tname_alt\n",
    "        pos = tname_alt.rfind(TBF)\n",
    "        if pos > 0: tname = tname_alt[0:pos]\n",
    "        t = relfieldindex[tname]\n",
    "        tname_rep = '{}_{}'.format(t, tname_alt)\n",
    "        main_id = '{}_id'.format(t)\n",
    "        val_id = '{}_id'.format(tname)\n",
    "        fdefs = '''\n",
    "    {mi} int(4),\n",
    "    {vi} int(4),\n",
    "    foreign key ({mi}) references {mt}(id),\n",
    "    foreign key ({vi}) references {tn}(id)\n",
    "'''.format(mt=t, mi=main_id, tn=tname, vi=val_id)\n",
    "        cf.write('''\n",
    "create table {} ({});\n",
    "'''.format(tname_rep, fdefs))\n",
    "        sql_data(df, tname_rep, [main_id, val_id], relxtables[tname_alt])\n",
    "    return errors\n",
    "\n",
    "\n",
    "def sql_export():\n",
    "    errors = 0\n",
    "    cf = open('{}/create.sql'.format(RESULT_DIR), 'w')\n",
    "    df = open('{}/data.sql'.format(RESULT_DIR), 'w')\n",
    "    df.write('''\n",
    "select \"FILL TABLES OF DATABASE {db}\" as \" \";\n",
    "\n",
    "use {db};\n",
    "\n",
    "'''.format(db=DB_NAME))\n",
    "\n",
    "    cf.write('''\n",
    "select \"CREATE DATABASE {db} AND TABLES\" as \" \";\n",
    "\n",
    "drop database if exists {db};\n",
    "create database {db} character set utf8;\n",
    "use {db};\n",
    "\n",
    "'''.format(db=DB_NAME))\n",
    "    cf.write('/* value tables */\\n')\n",
    "    df.write('\\n/* value tables */\\n')\n",
    "    errors += print_reltables(reltables, relvalues, cf, df)\n",
    "\n",
    "    cf.write('/* main tables */\\n')\n",
    "    df.write('\\n/* main tables */\\n')\n",
    "    errors += print_maintables(maindata, reltables, cf, df)\n",
    "\n",
    "    cf.write('/* cross tables */\\n')\n",
    "    df.write('\\n/* cross tables */\\n')\n",
    "    errors += print_relxtables(relxtables, cf, df)\n",
    "\n",
    "    cf.close()\n",
    "    df.close()\n",
    "    \n",
    "    if errors:\n",
    "        warning('There were {} errors'.format(errors), count=False)\n",
    "    else:\n",
    "        info('SQL OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================BEGIN PROCESSING================================\n",
      "==================================CHECK CONFIG==================================\n",
      "Configuration OK\n",
      "====================================READ FM=====================================\n",
      "Parsing contrib\n",
      "Parsing country\n",
      "Parsing help\n",
      "Parsing remark\n",
      "Parsing vcchead\n",
      "Parsing workinggroup\n",
      "================================MERGE pre CHECK=================================\n",
      "Merge definitions OK\n",
      "===============================FIELD DEFINITIONS================================\n",
      "Table contrib             : 60 fields\n",
      "Table country             :  2 fields\n",
      "Table help                :  2 fields\n",
      "Table remark              :  5 fields\n",
      "Table vcchead             :  2 fields\n",
      "Table workinggroup        : 27 fields\n",
      "Field definitions OK\n",
      "================================MERGE post CHECK================================\n",
      "Merge OK\n",
      "==================================SKIP FIELDS===================================\n",
      "Field skips OK\n",
      "===================================READ DATA====================================\n",
      "Table contrib             :  309 rows read\n",
      "Table country             :   24 rows read\n",
      "Table help                :    1 rows read\n",
      "Table remark              :  176 rows read\n",
      "Table vcchead             :   14 rows read\n",
      "Table workinggroup        :   20 rows read\n",
      "Data import OK\n",
      "=================================TRANSFORM ROWS=================================\n",
      "Table `contrib`:   309 rows checked\n",
      "Table `country`:    24 rows checked\n",
      "Table `help`:     1 rows checked\n",
      "Table `remark`:   176 rows checked\n",
      "Table `vcchead`:    14 rows checked\n",
      "Table `workinggroup`:    20 rows checked\n",
      "NB: contrib:costs_total \"1000000\" <= 1,000,000 \n",
      "\n",
      "NB: contrib:costs_total \"120000\" <= 120,000\n",
      "NB: contrib:costs_total \"1281.12\" <= 1281.12\n",
      "NB: contrib:costs_total \"12811.2\" <= 12811.2\n",
      "NB: contrib:costs_total \"143033.12\" <= 143,033.12 â‚¬\n",
      "NB: contrib:costs_total \"146345\" <= 146.345\n",
      "NB: contrib:costs_total \"158750\" <= 158,750\n",
      "NB: contrib:costs_total \"20000\" <= â‚¬ 20,000\n",
      "\n",
      "NB: contrib:costs_total \"214500.00\" <= 214,500.00\n",
      "NB: contrib:costs_total \"252400\" <= 252,400\n",
      "NB: contrib:costs_total \"3375\" <= 3,375\n",
      "NB: contrib:costs_total \"3437.75\" <= 3,437.75\n",
      "NB: contrib:costs_total \"347040.53\" <= 347040.53\n",
      "NB: contrib:costs_total \"36828.35\" <= 36,828.35\n",
      "NB: contrib:costs_total \"60170.00\" <= 60,170.00 â‚¬\n",
      "NB: contrib:costs_total \"68612.6\" <= 68,612.6\n",
      "NB: contrib:costs_total \"72064\" <= 72,064\n",
      "NB: contrib:costs_total \"73271.84\" <= 73,271.84\n",
      "NB: contrib:costs_total \"74797.60\" <= 74,797.60\n",
      "NB: contrib:costs_total \"80340.00\" <= 80,340.00\n",
      "NB: contrib:costs_total \"95959.20\" <= 95,959.20\n",
      "Money OK\n",
      "===================================PIVOT DATA===================================\n",
      "Table `contrib`: 309   records and 49 fields pivoted\n",
      "Table `country`: 24    records and 2  fields pivoted\n",
      "Table `help`: 1     records and 2  fields pivoted\n",
      "Table `remark`: 176   records and 5  fields pivoted\n",
      "Table `vcchead`: 14    records and 2  fields pivoted\n",
      "Table `workinggroup`: 20    records and 27 fields pivoted\n",
      "Pivot OK\n",
      "==================================MOVE FIELDS===================================\n",
      "moved fields\n",
      "\t`approved`\n",
      "\t`dateandtime_approval`\n",
      "\t`dateandtime_cioapproval`\n",
      "\t`dateandtime_ciozero`\n",
      "\t`submit`\n",
      "\t`vcc11_name`\n",
      "\t`vcc12_name`\n",
      "\t`vcc21_name`\n",
      "\t`vcc22_name`\n",
      "\t`vcc31_name`\n",
      "\t`vcc32_name`\n",
      "\t`vcc41_name`\n",
      "\t`vcc42_name`\n",
      "\t`vcc_head_decision`\n",
      "\t`vcc_head_decision_vcc11`\n",
      "\t`vcc_head_decision_vcc12`\n",
      "\t`vcc_head_decision_vcc21`\n",
      "\t`vcc_head_decision_vcc22`\n",
      "\t`vcc_head_decision_vcc31`\n",
      "\t`vcc_head_decision_vcc32`\n",
      "\t`vcc_head_decision_vcc41`\n",
      "\t`vcc_head_decision_vcc42`\n",
      "\t`vcchead_approval`\n",
      "\t`vcchead_disapproval`\n",
      "from `contrib` to `assessment`\n",
      "Move fields OK\n",
      "==================================REMODEL DATA==================================\n",
      "NB: table `contrib`: value field `country` will be linked to `country:countrycode\n",
      "Extraction OK\n",
      "================================FIX LINKED DATA=================================\n",
      "NB: linking `contrib:country` to table `country` on `countrycode`\n",
      "===================================WRITE SQL====================================\n",
      "NB: Field `description_of_contribution`: value with length 12856 gets type TEXT\n",
      "SQL OK\n",
      "=================================END PROCESSING=================================\n",
      "OK, no warnings\n"
     ]
    }
   ],
   "source": [
    "info('{:=^80}'.format('BEGIN PROCESSING'))\n",
    "resetw()\n",
    "\n",
    "info('{:=^80}'.format('CHECK CONFIG'))\n",
    "check_config()\n",
    "\n",
    "info('{:=^80}'.format('READ FM'))\n",
    "(root, main_tables_raw) = read_fm()\n",
    "\n",
    "info('{:=^80}'.format('MERGE pre CHECK'))\n",
    "merge_fields = check_merge()\n",
    "\n",
    "info('{:=^80}'.format('FIELD DEFINITIONS'))\n",
    "(tfields, field_defs_raw) = getfielddefs()\n",
    "\n",
    "info('{:=^80}'.format('MERGE post CHECK'))\n",
    "check_merge_more()\n",
    "\n",
    "info('{:=^80}'.format('SKIP FIELDS'))\n",
    "fields_raw = do_skips()\n",
    "\n",
    "info('{:=^80}'.format('READ DATA'))\n",
    "rows_raw = getdata()\n",
    "\n",
    "info('{:=^80}'.format('TRANSFORM ROWS'))\n",
    "rows = transformrows()\n",
    "\n",
    "info('{:=^80}'.format('PIVOT DATA'))\n",
    "field_data_raw = pivot()\n",
    "\n",
    "info('{:=^80}'.format('MOVE FIELDS'))\n",
    "(main_tables, fields, field_defs, field_data) = move_fields()\n",
    "\n",
    "info('{:=^80}'.format('REMODEL DATA'))\n",
    "(maindata, reltables, relxtables, relvalues, relfieldindex) = transform_data()\n",
    "\n",
    "info('{:=^80}'.format('FIX LINKED DATA'))\n",
    "fix_them()\n",
    "\n",
    "info('{:=^80}'.format('WRITE SQL'))\n",
    "sql_export()\n",
    "\n",
    "info('{:=^80}'.format('END PROCESSING'))\n",
    "finalw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pprintf(tname, fname):\n",
    "    values_raw = field_data[tname][fname]\n",
    "    values = sorted(v for v in reduce(set.union, values_raw, set()) if v != 'NULL')\n",
    "    print('\\n'.join('{}'.format(v) for v in values))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'AT'\n",
      "'BE'\n",
      "'DE'\n",
      "'FR'\n",
      "'GR'\n",
      "'HR'\n",
      "'IE'\n",
      "'IT'\n",
      "'LU'\n",
      "'NL'\n",
      "'RS'\n",
      "'SI'\n"
     ]
    }
   ],
   "source": [
    "pprintf('contrib', 'country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
