{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing InKind from FileMaker\n",
    "\n",
    "We use an XML export of the various tables in the FileMaker Inkind database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os,sys,re,collections\n",
    "from os.path import splitext, basename\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "from glob import glob\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration OK\n"
     ]
    }
   ],
   "source": [
    "# Locations\n",
    "\n",
    "HOME_DIR = os.path.expanduser('~').replace('\\\\', '/')\n",
    "BASE_DIR = '{}/projects/has/dacs'.format(HOME_DIR)\n",
    "FM_DIR = '{}/fm'.format(BASE_DIR)\n",
    "TEMP_DIR = '{}/tmp'.format(BASE_DIR)\n",
    "RESULT_DIR = '{}/sql'.format(BASE_DIR)\n",
    "FMNS = '{http://www.filemaker.com/fmpxmlresult}'\n",
    "ROW_RAW_FILE = '{}/row_raw_file'.format(TEMP_DIR)\n",
    "ROW_FILE = '{}/row_file'.format(TEMP_DIR)\n",
    "ROW_EXT = 'txt'\n",
    "MODEL_FILE = '{}/model.txt'.format(TEMP_DIR)\n",
    "\n",
    "# data type details\n",
    "\n",
    "MIN_M = 5       # minimum varchar size = 2**MIN_M\n",
    "MAX_M = 13      # maximum varchar size = 2**MAX_M\n",
    "LIMIT_ROWS = 50 # maximum number of rows to be written in one sql insert statement\n",
    "TYPES = {'number', 'text', 'valuta', 'date', 'datetime'}\n",
    "DATE_PATTERN = re.compile(\n",
    "    '^\\s*([0-9]{2})/([0-9]{2})/([0-9]{4})$'\n",
    ")\n",
    "DATE2_PATTERN = re.compile(\n",
    "    '^\\s*([0-9]{4})-([0-9]{2})-([0-9]{2})$'\n",
    ")\n",
    "DATETIME_PATTERN = re.compile(\n",
    "    '^\\s*([0-9]{2})/([0-9]{2})/([0-9]{4})\\s+([0-9]{2}):([0-9]{2})(?::([0-9]{2}))?$'\n",
    ")\n",
    "NULL_VALUES = {\n",
    "    'http://',\n",
    "    'https://',\n",
    "}\n",
    "\n",
    "splitters = dict(\n",
    "    generic=re.compile('[ \\t]*[\\n+][ \\t\\n]*'),\n",
    "    generic_comma=re.compile('[ \\t]*[\\n+,][ \\t\\n]*'),\n",
    ")\n",
    "\n",
    "good = True\n",
    "for x in [1]:\n",
    "    good = False\n",
    "    if not os.path.exists(BASE_DIR):\n",
    "        print('BASE_DIR does not exist: {}'.format(BASE_DIR))\n",
    "        break\n",
    "    this_good = True\n",
    "    for cdir in (TEMP_DIR, RESULT_DIR):\n",
    "        this_good = False\n",
    "        if not os.path.exists(cdir):\n",
    "            try:\n",
    "                os.makedirs(cdir)\n",
    "            except os.error as e:\n",
    "                print('{} could not be created.'.format(cdir))\n",
    "                break\n",
    "        this_good = True\n",
    "    if not this_good:\n",
    "        break\n",
    "    good = True\n",
    "if not good:\n",
    "    print('There were configuration errors')\n",
    "else:\n",
    "    print('Configuration OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value validation and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def date_repl(match):\n",
    "    [d,m,y] = list(match.groups())\n",
    "    return '{}-{}-{}'.format(y,m,d)\n",
    "    \n",
    "def date2_repl(match):\n",
    "    [y,m,d] = list(match.groups())\n",
    "    return '{}-{}-{}'.format(y,m,d)\n",
    "    \n",
    "def datetime_repl(match):\n",
    "    [d,m,y,hr,mn,sc] = list(match.groups())\n",
    "    return '{}-{}-{}T{}:{}:{}'.format(y,m,d,hr,mn,sc or '00')\n",
    "    \n",
    "def sq(v_raw):\n",
    "    return \"'{}'\".format(\n",
    "        v_raw.strip().replace(\"'\",\"''\").replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "    )\n",
    "\n",
    "def num(v_raw, i, t, fname):\n",
    "    if v_raw.isdigit(): return int(v_raw)\n",
    "    print(\n",
    "        'WARNING: table `{}` field `{}` record {}: not an integer: \"{}\"'.format(\n",
    "            t, fname, i, v_raw\n",
    "    ))\n",
    "    return v_raw\n",
    "\n",
    "money_warnings = {}\n",
    "\n",
    "def money(v_raw, i, t, fname):\n",
    "    warn = ',' in v_raw or '.' in v_raw\n",
    "    v = v_raw.strip().replace(' ','').replace('â‚¬', '').replace('\\u00a0', '')\n",
    "    for p in range(2,4): # interpret . or , as decimal point if less than 3 digits follow it\n",
    "        if len(v) >= p and v[-p] in '.,': \n",
    "            v_i = v[::-1]\n",
    "            if v_i[p-1] == ',': v_i = v_i.replace(',', 'D', 1)\n",
    "            elif v_i[p-1] == '.': v_i = v_i.replace('.', 'D', 1)\n",
    "            v = v_i[::-1]\n",
    "    v = v.replace('.','').replace(',','')\n",
    "    v = v.replace('D', '.')\n",
    "    if not v.replace('.','').isdigit():\n",
    "        print(\n",
    "            'WARNING: table `{}` field `{}` record {}: not a decimal number: \"{}\" <= \"{}\"'.format(\n",
    "                t, fname, i, v, v_raw,\n",
    "        ))\n",
    "        money_warnings.setdefault('{}:{}'.format(t, fname), {}).setdefault(v, set()).add(v_raw)\n",
    "    elif warn:\n",
    "        money_warnings.setdefault('{}:{}'.format(t, fname), {}).setdefault(v, set()).add(v_raw)\n",
    "    return v\n",
    "\n",
    "def dt(v_raw, i, t, fname):\n",
    "    if not DATE2_PATTERN.match(v_raw):\n",
    "        print(\n",
    "            'WARNING: table `{}` field `{}` record {}: not a valid date: \"{}\"'.format(\n",
    "                t, fname, i, v_raw\n",
    "        ))\n",
    "        return v_raw\n",
    "    return(\"'{}'\".format(DATE2_PATTERN.sub(date2_repl, v_raw)))\n",
    "\n",
    "def dtm(v_raw, i, t, fname):\n",
    "    if not DATETIME_PATTERN.match(v_raw):\n",
    "        print(\n",
    "            'WARNING: table `{}` field `{}` record {}: not a valid date time: \"{}\"'.format(\n",
    "                t, fname, i, v_raw\n",
    "        ))\n",
    "        return v_raw\n",
    "    return(\"'{}'\".format(DATETIME_PATTERN.sub(datetime_repl, v_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_tables = []\n",
    "\n",
    "def read_fm():\n",
    "    main_tables.clear()\n",
    "    parser = etree.XMLParser(remove_blank_text=True, ns_clean=True)\n",
    "    root = {}\n",
    "    for infile in glob('{}/*.xml'.format(FM_DIR)):\n",
    "        tname = basename(splitext(infile)[0])\n",
    "        print('Parsing {}'.format(tname))\n",
    "        root[tname] = etree.parse(infile, parser).getroot()\n",
    "        main_tables.append(tname)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fields and their types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DB_NAME = 'inkind_data'\n",
    "\n",
    "FIELD_TYPE_OVERRIDE = dict(\n",
    "    contrib=dict(\n",
    "        costs_total='valuta',\n",
    "        total_costs_total='valuta',\n",
    "        whois='text',\n",
    "        creation_date_time='datetime',\n",
    "        modification_date_time='datetime',\n",
    "        dateandtime_approval='datetime',\n",
    "        dateandtime_cioapproval='datetime',\n",
    "        dateandtime_ciozero='datetime',\n",
    "    )\n",
    ")\n",
    "\n",
    "SPLIT_FIELDS = dict(\n",
    "    contrib=dict(\n",
    "        disciplines_associated=splitters['generic'],\n",
    "        other_keywords=splitters['generic_comma'],\n",
    "        tadirah_research_activities=splitters['generic'],\n",
    "        tadirah_research_objects=splitters['generic'],\n",
    "        tadirah_research_techniques=splitters['generic'],\n",
    "        type_of_inkind=splitters['generic'],\n",
    "        vcc=splitters['generic'],\n",
    "    )\n",
    ")\n",
    "\n",
    "MERGE_FIELDS = dict(\n",
    "    contrib=dict(\n",
    "        academic_entity_url=('academic_entity_url_2',),\n",
    "        contribution_url=('contribution_url_2',),\n",
    "        contact_person_mail=('contact_person_mail_2',),\n",
    "        gnewpassword=('gnewpassword2',),\n",
    "        vcc_head_decision_vcc1=('vcc_head_decision_vcc11', 'vcc_head_decision_vcc12'),\n",
    "        vcc_head_decision_vcc2=('vcc_head_decision_vcc21', 'vcc_head_decision_vcc22'),\n",
    "        vcc_head_decision_vcc3=('vcc_head_decision_vcc31', 'vcc_head_decision_vcc32'),\n",
    "        vcc_head_decision_vcc4=('vcc_head_decision_vcc41', 'vcc_head_decision_vcc42'),\n",
    "        vcc_name=(\n",
    "            'vcc11_name', 'vcc12_name', \n",
    "            'vcc21_name', 'vcc22_name',\n",
    "            'vcc31_name', 'vcc32_name',\n",
    "            'vcc41_name', 'vcc42_name',\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "SKIP_FIELDS = dict(\n",
    "    contrib={\n",
    "        'teller',\n",
    "        'whois',\n",
    "        'help_text',\n",
    "        'help_description',\n",
    "    },\n",
    ")\n",
    "\n",
    "VALUE_FIELDS = dict(\n",
    "    contrib={\n",
    "        'country',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process field specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_merge():\n",
    "    merge_errors = 0\n",
    "    merge_fields = {}\n",
    "\n",
    "    for t in main_tables:\n",
    "        for (mfhead, mftail) in MERGE_FIELDS.get(t, {}).items():\n",
    "            for f in mftail:\n",
    "                if f in merge_fields.get(t, {}):\n",
    "                    print(\n",
    "                        'WARNING: table `{}` field `{}` already merged into `{}` now to be merged into `{}`'.format(\n",
    "                            t, f, merge_fields[t][f], mfhead,\n",
    "                    ))\n",
    "                    merge_errors += 1\n",
    "                merge_fields.setdefault(t, {})[f] = mfhead\n",
    "    if merge_errors:\n",
    "        print('There were {} merge errors'.format(merge_errors))\n",
    "    else:\n",
    "        print('Merge definitions OK')\n",
    "    return merge_fields\n",
    "\n",
    "def check_merge_more():\n",
    "    merge_errors = 0\n",
    "    for t in main_tables:\n",
    "        for f in merge_fields.get(t, {}):\n",
    "            if f not in field_defs[t]:\n",
    "                print(\n",
    "                    'WARNING: table `{}`: cannot merge unknown field `{}`'.format(\n",
    "                    t, f,\n",
    "                ))\n",
    "                merge_errors += 1\n",
    "                continue\n",
    "            ftarget = merge_fields[t][f]\n",
    "            (ftype, fmult) = field_defs[t][f]\n",
    "            if ftarget not in field_defs[t]:\n",
    "                field_defs[t][ftarget] = [ftype, 0]\n",
    "            (ttype, tmult) = field_defs[t][ftarget]\n",
    "            if ttype != ftype:\n",
    "                print(\n",
    "                    'WARNING: table `{}` field `{}` of type \"{}\" is merged into field `{}` of other type \"{}\"'.format(\n",
    "                        t, f, ftype, ftarget, ttype,\n",
    "                ))\n",
    "                merge_errors += 1\n",
    "            field_defs[t][ftarget][1] += fmult\n",
    "            del field_defs[t][f]\n",
    "    if merge_errors:\n",
    "        print('There were {} merge errors'.format(merge_errors))\n",
    "    else:\n",
    "        print('Merge OK')\n",
    "\n",
    "def getfielddefs():\n",
    "    field_defs = {}\n",
    "    fd_errors = 0\n",
    "    tfields = {}\n",
    "    for t in main_tables:\n",
    "        fieldroots = [x for x in root[t].iter(FMNS+'METADATA')]\n",
    "        fieldroot = fieldroots[0]\n",
    "        tfields[t] = []\n",
    "        for x in fieldroot.iter(FMNS+'FIELD'):\n",
    "            fname = x.get('NAME').lower().replace(' ','_')\n",
    "            ftype = FIELD_TYPE_OVERRIDE.\\\n",
    "                get(t, {}).\\\n",
    "                get(fname, None) or x.get('TYPE').lower()\n",
    "            fmult = int(x.get('MAXREPEAT'))\n",
    "            if fname in SPLIT_FIELDS.get(t, {}): fmult += 1\n",
    "            tfields[t].append(fname)\n",
    "            field_defs.setdefault(t, {})[fname] = [ftype, fmult]\n",
    "            if ftype not in TYPES:\n",
    "                print('WARNING: table `{}` field `{}` has unknown type \"{}\"'.format(\n",
    "                    t, fname, ftype,\n",
    "                ))\n",
    "                fd_errors += 1\n",
    "        print('Table {:<20}: {:>2} fields'.format(t, len(tfields[t])))\n",
    "    if fd_errors:\n",
    "        print('There were {} field definition errors'.format(fd_errors))\n",
    "    else:\n",
    "        print('Field definitions OK')\n",
    "    return (tfields, field_defs)\n",
    "\n",
    "def do_skips():\n",
    "    fields = {}\n",
    "    for t in main_tables:\n",
    "        for f in SKIP_FIELDS.get(t, set()): del field_defs[t][f]\n",
    "        fields[t] = sorted(\n",
    "            set(field_defs[t].keys()) | set(merge_fields.get(t, {}).values())\n",
    "        )\n",
    "    return fields\n",
    "\n",
    "def report_model():\n",
    "    mf = open(MODEL_FILE, 'w')\n",
    "    for t in main_tables:\n",
    "        mf.write('[{}] ({} fields)\\n\\t{}\\n\\n'.format(\n",
    "            t,\n",
    "            len(fields[t]), \n",
    "            '\\n\\t'.join('{:<8}{} {}'.format(\n",
    "                *field_defs[t][f], \n",
    "                f, \n",
    "            ) for f in fields[t])\n",
    "        ))\n",
    "        print('Table {:<20}: {:>2} retained fields'.format(\n",
    "            t,\n",
    "            len(fields[t]), \n",
    "        ))\n",
    "    mf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getdata():\n",
    "    rows_raw = {}\n",
    "    errors = {}\n",
    "\n",
    "    for t in main_tables:\n",
    "        dataroots = [x for x in root[t].iter(FMNS+'RESULTSET')]\n",
    "        dataroot = dataroots[0]\n",
    "        rows_raw[t] = []\n",
    "\n",
    "        for (i, r) in enumerate(dataroot.iter(FMNS+'ROW')):\n",
    "            row = []\n",
    "            for c in r.iter(FMNS+'COL'):\n",
    "                data = [x.text for x in c.iter(FMNS+'DATA')]\n",
    "                row.append(data)\n",
    "            if len(row) != len(tfields[t]):\n",
    "                errors.setdefault(t, {}).setdefault('Number of fields', []).append(i)\n",
    "            rows_raw[t].append(row)\n",
    "\n",
    "        rf = open('{}_{}.{}'.format(ROW_RAW_FILE, t, ROW_EXT), 'w')\n",
    "        for row in rows_raw[t]:\n",
    "            for (fname, values) in zip(tfields[t], row):\n",
    "                rf.write('@{:>30} = {}\\n'.format(\n",
    "                    fname,\n",
    "                    ' | '.join('{}'.format(v) for v in values),\n",
    "                ))\n",
    "            rf.write('{}\\n'.format('='*100))\n",
    "        rf.close()\n",
    "        print('Table {:<20}: {:>4} rows read'.format(t, len(rows_raw[t])))\n",
    "    if errors:\n",
    "        for t in sorted(errors):\n",
    "            for k in sorted(errors[t]):\n",
    "                print('Table {:<20}: {:<20}: {}'.format(t, k, ','.join(errors[k])))\n",
    "    else:\n",
    "        print('Data import OK')\n",
    "    return rows_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the values\n",
    "\n",
    "Various non-informational values will be converted to NULL.\n",
    "Values will be thinned: \n",
    "Identical values will be reduced to one copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transformrows():\n",
    "    rows = {}\n",
    "    money_warnings.clear()\n",
    "    for t in main_tables:\n",
    "        for (i, row_raw) in enumerate(rows_raw.get(t, [])):\n",
    "            values = {}\n",
    "            for (fname, values_raw) in zip(tfields[t], row_raw):\n",
    "                if fname in SKIP_FIELDS.get(t, set()): continue\n",
    "                sep = SPLIT_FIELDS.get(t, {}).get(fname, None)\n",
    "                if sep != None:\n",
    "                    values_raw = sorted(reduce(\n",
    "                        set.union, \n",
    "                        [set(sep.split(v)) for v in values_raw if v != None], \n",
    "                        set(),\n",
    "                    ))\n",
    "                    if '' in values_raw: values_raw.remove('')\n",
    "                ftarget = merge_fields.get(t, {}).get(fname, fname)\n",
    "                (ftype, fmult) = field_defs[t][ftarget]\n",
    "                valset = set()\n",
    "                for v_raw in values_raw:\n",
    "                    if v_raw == None or v_raw in NULL_VALUES: v = 'NULL'\n",
    "                    elif ftype == 'text': v = sq(v_raw)\n",
    "                    elif ftype == 'number': v = num(v_raw, i, t, fname)\n",
    "                    elif ftype == 'valuta': v = money(v_raw, i, t, fname)\n",
    "                    elif ftype == 'date': v = dt(v_raw, i, t, fname)\n",
    "                    elif ftype == 'datetime': v = dtm(v_raw, i, t, fname)\n",
    "                    else: v = v_raw\n",
    "                    valset.add(v)\n",
    "                if fmult > 1: valset.discard('NULL')\n",
    "                these_values = values.setdefault(ftarget, set())\n",
    "                these_values |= valset\n",
    "            rows.setdefault(t, []).append(values)\n",
    "        print('Table `{}`: {:>5} rows checked'.format(t, len(rows[t])))\n",
    "\n",
    "        rf = open('{}_{}.{}'.format(ROW_FILE, t, ROW_EXT), 'w')\n",
    "        for row in rows[t]:\n",
    "            for (fname, values) in sorted(row.items()):\n",
    "                rf.write('@{:>30} = {}\\n'.format(\n",
    "                    fname,\n",
    "                    ' | '.join('{}'.format(v) for v in sorted(values)),\n",
    "                ))\n",
    "            rf.write('{}\\n'.format('='*100))\n",
    "        rf.close()\n",
    "\n",
    "    if money_warnings:\n",
    "        for tf in sorted(money_warnings):\n",
    "            for v in sorted(money_warnings[tf]):\n",
    "                print('WARNING: {} \"{}\" <= {}'.format(\n",
    "                    tf, v,\n",
    "                    ' | '.join(money_warnings[tf][v]),\n",
    "            ))\n",
    "    else:\n",
    "        print('Money OK')\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn the data into a dict\n",
    "\n",
    "We represent the data with a dictionary. The keys are the field names.\n",
    "The values are dictionaries again, with keys new ids and with values the value that the row with that id has for that field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pivot():\n",
    "    field_data = {}\n",
    "    for t in main_tables:\n",
    "        for row in rows[t]:\n",
    "            for (fname, values) in sorted(row.items()):\n",
    "                field_data.setdefault(t, {}).setdefault(fname, []).append(values)\n",
    "        print('Table `{}`: {:<5} records and {:<2} fields pivoted'.format(\n",
    "            t, len(rows[t]), len(field_data[t]),\n",
    "        ))\n",
    "\n",
    "    # check\n",
    "    good = True\n",
    "    for t in field_data:\n",
    "        for f in field_data[t]:\n",
    "            if len(field_data[t][f]) != len(rows[t]):        \n",
    "                print(\n",
    "                    'WARNING: table `{}`, field `{}`: wrong number of records: {} instead of {}'.format(\n",
    "                        t, f, len(field_data[t][f]), len(rows[t]),\n",
    "                ))\n",
    "                good = False\n",
    "    if good:\n",
    "        print('OK')\n",
    "    else:\n",
    "        print('There were errors')\n",
    "    return field_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract related tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(t, fname, maindata, relvalues, relindex, reltables, relxtables, relfieldindex):\n",
    "    is_single = field_defs[t][fname][1] == 1 # single value of multiple values\n",
    "    error = False\n",
    "    if fname in relfieldindex:\n",
    "        print('WARNING: Related table `{}` extracted from `{}` and earlier from `{}`'.format(\n",
    "            fname, t, relfieldindex[fname],\n",
    "        ))\n",
    "        error = True\n",
    "    relfieldindex[fname] = t\n",
    "    for (i, values) in enumerate(field_data[t][fname]):\n",
    "        for value in values:\n",
    "            vid = relvalues.setdefault(fname, {}).get(value, None)\n",
    "            if vid == None:\n",
    "                relindex[fname] += 1\n",
    "                vid = relindex[fname]\n",
    "                reltables.setdefault(fname, []).append((vid, value))\n",
    "            relvalues[fname][value] = vid\n",
    "            if is_single:\n",
    "                maindata[t][fname][i] = [vid]\n",
    "            else:\n",
    "                relxtables.setdefault(fname, []).append((i, vid))\n",
    "    if not is_single: del maindata[t][fname]\n",
    "    return error\n",
    "\n",
    "def transform_data():\n",
    "    maindata = deepcopy(field_data)\n",
    "    relvalues = {} # dicts\n",
    "    relindex = collections.Counter()\n",
    "    reltables = {} # lists\n",
    "    relxtables = {} # lists\n",
    "    relfieldindex = {}\n",
    "    errors = 0\n",
    "    for t in main_tables:\n",
    "        field_list =\\\n",
    "            VALUE_FIELDS.get(t, set()) |\\\n",
    "            {f for f in fields[t] if field_defs[t][f][1] > 1}\n",
    "        for fname in field_list:\n",
    "            if fname not in field_defs[t]:\n",
    "                print('ERROR: table `{}`: wrong field {}'.format(t, fname))\n",
    "                continue\n",
    "            error = extract(t, fname, maindata, relvalues, relindex, reltables, relxtables, relfieldindex)\n",
    "            if error: errors +=1\n",
    "    if errors:\n",
    "        print('There were {} extraction errors'.format(errors))\n",
    "    else:\n",
    "        print('Extraction OK')\n",
    "    return (maindata, reltables, relxtables, relvalues, relfieldindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write sql to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getsize(source, fname):\n",
    "    values = set()\n",
    "    for vals in source: values |= set(vals)\n",
    "    maxlen = max({len(x) for x in values if x != 'NULL'}, default=0)\n",
    "    result = 0\n",
    "    for m in range(MIN_M, MAX_M+1):\n",
    "        if maxlen <= 2**m:\n",
    "            result = m\n",
    "            break\n",
    "    if maxlen > 2**MAX_M:\n",
    "        print(\n",
    "            'Field `{}`: value with length {} gets type TEXT'.format(\n",
    "                fname, maxlen, 2**MAX_M,\n",
    "        ))\n",
    "        return False\n",
    "    return 2**m\n",
    "\n",
    "def getdef(source, t, fname, newfname, warn_mult=True):\n",
    "    (ft, fmult) = field_defs[t][fname]\n",
    "    if warn_mult and fmult > 1:\n",
    "        print(\n",
    "            'WARNING: skipping field `{}` because it contains multiple values'.format(\n",
    "                fname,\n",
    "        ))\n",
    "        return None\n",
    "    if ft == 'number':\n",
    "        ftype = 'int'\n",
    "        fsize = '(4)'\n",
    "        fext = ''\n",
    "    elif ft == 'text':\n",
    "        ftype = 'varchar'\n",
    "        fsize_raw = getsize(source, fname)\n",
    "        if not fsize_raw:\n",
    "            ftype = 'text'\n",
    "            fsize = ''\n",
    "        else:\n",
    "            fsize = '({})'.format(fsize_raw)\n",
    "        fext = 'character set utf8'\n",
    "    elif ft == 'valuta':\n",
    "        ftype = 'decimal'\n",
    "        fsize = '(10,2)'\n",
    "        fext = ''\n",
    "    elif ft == 'date':\n",
    "        ftype = 'datetime'\n",
    "        fsize = ''\n",
    "        fext = ''\n",
    "    elif ft == 'datetime':\n",
    "        ftype = 'datetime'\n",
    "        fsize = ''\n",
    "        fext = ''\n",
    "    else:\n",
    "        print('WARNING: skipping field `{}` because it has unknown type `{}`'.format(\n",
    "            fname, ft,\n",
    "        ))\n",
    "        return None\n",
    "    return '{} {}{} {}'.format(newfname, ftype, fsize, fext)\n",
    "\n",
    "def getrdef(fname):\n",
    "    return '''{fn}_id int(4),\n",
    "    foreign key ({fn}_id) references {fn}(id)'''.format(fn=fname)\n",
    "\n",
    "def sql_data(df, tname, flist, rows):\n",
    "    head = 'insert into {} ({}) values'.format(tname, ','.join(flist))\n",
    "    for (i, row) in enumerate(rows):\n",
    "        if i % LIMIT_ROWS == 0:\n",
    "            if i > 0: df.write(';')\n",
    "            df.write('\\n')\n",
    "            df.write('select \"table {} row {}\" as \" \";\\n'.format(tname, i))\n",
    "            df.write(head)\n",
    "            sep = ''\n",
    "        df.write('\\n{}\\t'.format(sep))\n",
    "        sep = ','\n",
    "        df.write('({})'.format(','.join(str(x) for x in row)))\n",
    "    df.write(';\\n')\n",
    "        \n",
    "def print_maintables(maindata, reltables, cf, df):\n",
    "    for t in maindata:\n",
    "        fdefs = ['id int(4) primary key']\n",
    "        flist = sorted(maindata[t])\n",
    "        fnewlist = []\n",
    "        for fname in flist:\n",
    "            if fname in reltables:\n",
    "                fdef = getrdef(fname)\n",
    "                fnewname = '{}_id'.format(fname)\n",
    "            else:\n",
    "                fdef = getdef(field_data[t][fname], t, fname, fname)\n",
    "                fnewname = fname\n",
    "            fdefs.append(fdef)\n",
    "            fnewlist.append(fnewname)\n",
    "        cf.write('''\n",
    "create table {} (\n",
    "    {}\n",
    ");\n",
    "    '''.format(t, ',\\n\\t'.join(fdefs)))\n",
    "        maintable_raw = zip(*(maindata[t][f] for f in flist))\n",
    "        maintable = [\n",
    "            [i]+[sorted(vals)[0] for vals in row] for (i, row) in enumerate(maintable_raw)\n",
    "        ]\n",
    "        sql_data(df, t, ['id'] + fnewlist, maintable)\n",
    "\n",
    "def print_reltables(reltables, relvalues, cf, df):\n",
    "    for tname in sorted(reltables):\n",
    "        fdefs = ['id int(4) primary key']\n",
    "        fdef = getdef(\n",
    "            [relvalues[tname].keys()], \n",
    "            relfieldindex[tname], \n",
    "            tname, 'val', warn_mult=False,\n",
    "        )\n",
    "        if fdef == None: continue            \n",
    "        fdefs.append(fdef)\n",
    "        cf.write('''\n",
    "create table {} (\n",
    "    {}\n",
    ");\n",
    "'''.format(tname, ',\\n\\t'.join(fdefs)))\n",
    "        sql_data(df, tname, ['id', 'val'], reltables[tname])\n",
    "\n",
    "def print_relxtables(relxtables, cf, df):\n",
    "    for tname in sorted(relxtables):\n",
    "        t = relfieldindex[tname]\n",
    "        tname_rep = '{}_{}'.format(t, tname)\n",
    "        main_id = '{}_id'.format(t)\n",
    "        val_id = '{}_id'.format(tname)\n",
    "        fdefs = '''\n",
    "    {mi} int(4),\n",
    "    {vi} int(4),\n",
    "    foreign key ({mi}) references {mt}(id),\n",
    "    foreign key ({vi}) references {tn}(id)\n",
    "'''.format(mt=t, mi=main_id, tn=tname, vi=val_id)\n",
    "        cf.write('''\n",
    "create table {} ({});\n",
    "'''.format(tname_rep, fdefs))\n",
    "        sql_data(df, tname_rep, [main_id, val_id], relxtables[tname])\n",
    "\n",
    "\n",
    "def sql_export():\n",
    "    cf = open('{}/create.sql'.format(RESULT_DIR), 'w')\n",
    "    df = open('{}/data.sql'.format(RESULT_DIR), 'w')\n",
    "    df.write('''\n",
    "select \"FILL TABLES OF DATABASE {db}\" as \" \";\n",
    "\n",
    "use {db};\n",
    "\n",
    "'''.format(db=DB_NAME))\n",
    "\n",
    "    cf.write('''\n",
    "select \"CREATE DATABASE {db} AND TABLES\" as \" \";\n",
    "\n",
    "drop database if exists {db};\n",
    "create database {db} character set utf8;\n",
    "use {db};\n",
    "\n",
    "'''.format(db=DB_NAME))\n",
    "    cf.write('/* value tables */\\n')\n",
    "    df.write('\\n/* value tables */\\n')\n",
    "    print_reltables(reltables, relvalues, cf, df)\n",
    "\n",
    "    cf.write('/* main tables */\\n')\n",
    "    df.write('\\n/* main tables */\\n')\n",
    "    print_maintables(maindata, reltables, cf, df)\n",
    "\n",
    "    cf.write('/* cross tables */\\n')\n",
    "    df.write('\\n/* cross tables */\\n')\n",
    "    print_relxtables(relxtables, cf, df)\n",
    "\n",
    "    cf.close()\n",
    "    df.close()\n",
    "    print('SQL written')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing contrib\n",
      "Parsing country\n",
      "Parsing help\n",
      "Parsing remark\n",
      "Parsing vcc\n",
      "Parsing workinggroup\n",
      "Merge definitions OK\n",
      "Table contrib             : 60 fields\n",
      "Table country             :  2 fields\n",
      "Table help                :  2 fields\n",
      "Table remark              :  5 fields\n",
      "Table vcc                 :  2 fields\n",
      "Table workinggroup        : 27 fields\n",
      "Field definitions OK\n",
      "Merge OK\n",
      "Table contrib             : 41 retained fields\n",
      "Table country             :  2 retained fields\n",
      "Table help                :  2 retained fields\n",
      "Table remark              :  5 retained fields\n",
      "Table vcc                 :  2 retained fields\n",
      "Table workinggroup        : 27 retained fields\n",
      "Table contrib             :  309 rows read\n",
      "Table country             :   24 rows read\n",
      "Table help                :    1 rows read\n",
      "Table remark              :  176 rows read\n",
      "Table vcc                 :   14 rows read\n",
      "Table workinggroup        :   20 rows read\n",
      "Data import OK\n",
      "Table `contrib`:   309 rows checked\n",
      "Table `country`:    24 rows checked\n",
      "Table `help`:     1 rows checked\n",
      "Table `remark`:   176 rows checked\n",
      "Table `vcc`:    14 rows checked\n",
      "Table `workinggroup`:    20 rows checked\n",
      "WARNING: contrib:costs_total \"1000000\" <= 1,000,000 \n",
      "\n",
      "WARNING: contrib:costs_total \"120000\" <= 120,000\n",
      "WARNING: contrib:costs_total \"1281.12\" <= 1281.12\n",
      "WARNING: contrib:costs_total \"12811.2\" <= 12811.2\n",
      "WARNING: contrib:costs_total \"143033.12\" <= 143,033.12 â‚¬\n",
      "WARNING: contrib:costs_total \"146345\" <= 146.345\n",
      "WARNING: contrib:costs_total \"158750\" <= 158,750\n",
      "WARNING: contrib:costs_total \"20000\" <= â‚¬ 20,000\n",
      "\n",
      "WARNING: contrib:costs_total \"214500.00\" <= 214,500.00\n",
      "WARNING: contrib:costs_total \"252400\" <= 252,400\n",
      "WARNING: contrib:costs_total \"3375\" <= 3,375\n",
      "WARNING: contrib:costs_total \"3437.75\" <= 3,437.75\n",
      "WARNING: contrib:costs_total \"347040.53\" <= 347040.53\n",
      "WARNING: contrib:costs_total \"36828.35\" <= 36,828.35\n",
      "WARNING: contrib:costs_total \"60170.00\" <= 60,170.00 â‚¬\n",
      "WARNING: contrib:costs_total \"68612.6\" <= 68,612.6\n",
      "WARNING: contrib:costs_total \"72064\" <= 72,064\n",
      "WARNING: contrib:costs_total \"73271.84\" <= 73,271.84\n",
      "WARNING: contrib:costs_total \"74797.60\" <= 74,797.60\n",
      "WARNING: contrib:costs_total \"80340.00\" <= 80,340.00\n",
      "WARNING: contrib:costs_total \"95959.20\" <= 95,959.20\n",
      "WARNING: contrib:total_costs_total \"14588130.9\" <= 14588130.9\n",
      "Table `contrib`: 309   records and 41 fields pivoted\n",
      "Table `country`: 24    records and 2  fields pivoted\n",
      "Table `help`: 1     records and 2  fields pivoted\n",
      "Table `remark`: 176   records and 5  fields pivoted\n",
      "Table `vcc`: 14    records and 2  fields pivoted\n",
      "Table `workinggroup`: 20    records and 27 fields pivoted\n",
      "OK\n",
      "Extraction OK\n",
      "Field `description_of_contribution`: value with length 12856 gets type TEXT\n",
      "SQL written\n"
     ]
    }
   ],
   "source": [
    "root = read_fm()\n",
    "merge_fields = check_merge()\n",
    "(tfields, field_defs) = getfielddefs()\n",
    "check_merge_more()\n",
    "fields = do_skips()\n",
    "report_model()\n",
    "rows_raw = getdata()\n",
    "rows = transformrows()\n",
    "field_data = pivot()\n",
    "(maindata, reltables, relxtables, relvalues, relfieldindex) = transform_data()\n",
    "sql_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pprintf(tname, fname):\n",
    "    values_raw = field_data[tname][fname]\n",
    "    values = sorted(v for v in reduce(set.union, values_raw, set()) if v != 'NULL')\n",
    "    print('\\n'.join('{}'.format(v) for v in values))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dirk WintergrÃ¼n'\n",
      "'Hansmichael Hohenegger'\n",
      "'Hella Hollander'\n",
      "'Marianne Huan'\n",
      "'Sophie David'\n",
      "'Susan Schreibman'\n",
      "'Tibor KÃ¡lmÃ¡n'\n"
     ]
    }
   ],
   "source": [
    "pprintf('contrib', 'vcc_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
