{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing InKind from FileMaker\n",
    "\n",
    "We use an XML export of the various tables in the FileMaker Inkind database.\n",
    "\n",
    "The XML will be read, field definitions will be extracted from it, the data will be read.\n",
    "We do the following:\n",
    "* adapt the table and field organization;\n",
    "* adjust the field types and the values, especially for datetime and currency;\n",
    "* generate value tables and cross tables;\n",
    "* add extra information for countries, so that they can be visualized on a map\n",
    "* link values to existing tables;\n",
    "* write SQL create statements and insert statements\n",
    "* import a moderately denormalized version of the data into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os,sys,re,collections,json\n",
    "from os.path import splitext, basename\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "from glob import glob\n",
    "from lxml import etree\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra data\n",
    "Here we define extra data that will be used to enrich some tables later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "country_extra = dict(\n",
    "    AT=('Austria', 47.7, 15.11),\n",
    "    BE=('Belgium', 51.3, 3.1),\n",
    "    HR=('Croatia', 44.7, 15.6),\n",
    "    CY=('Cyprus', 35.0, 32.8),\n",
    "    CZ=('Czech Republic', 49.8, 15.2),\n",
    "    DK=('Denmark', 55.6, 11.0),\n",
    "    EE=('Estonia', 59.0, 25.0),\n",
    "    FR=('France', 46.5, 1.9),\n",
    "    DE=('Germany', 51.0, 10.4),\n",
    "    GR=('Greece', 38.0, 23.8),\n",
    "    HU=('Hungary', 46.9, 19.8),\n",
    "    IE=('Ireland', 53.1, -8.4),\n",
    "    IT=('Italy', 41.6, 13.0),\n",
    "    LV=('Latvia', 56.9, 26.8),\n",
    "    LT=('Lithuania', 55.2, 24.9),\n",
    "    LU=('Luxembourg', 49.6, 6.1),\n",
    "    MT=('Malta', 35.9, 14.4),\n",
    "    NL=('Netherlands', 52.8, 5.8),\n",
    "    PL=('Poland', 52.3, 19.8),\n",
    "    PT=('Portugal', 38.7, -9.0),\n",
    "    RS=('Serbia', 44.0, 20.8),\n",
    "    SK=('Slovakia', 48.8, 19.9),\n",
    "    SI=('Slovenia', 46.2, 14.4),\n",
    "    CH=('Switzerland', 46.9, 8.3),\n",
    "    GB=('United Kingdom', 52.9, -1.8),\n",
    "\n",
    "    AL=('Albania', 41.1, 19.9),\n",
    "    AD=('Andorra', 42.5, 1.6),\n",
    "    BY=('Belarus', 53.8, 29.2),\n",
    "    BA=('Bosnia and Herzegovina', 44.2, 18.2),\n",
    "    BG=('Bulgaria', 42.9, 26.5),\n",
    "    FI=('Finland', 63.3, 27.6),\n",
    "    GE=('Georgia', 41.66, 43.68),\n",
    "    IS=('Iceland', 65.0, -18.8),\n",
    "    SM=('San Marino', 43.8, 12.3),\n",
    "    KS=('Kosovo', 43.2, 21.9),\n",
    "    LI=('Liechtenstein', 47.2, 9.4),\n",
    "    MK=('Macedonia', 41.6, 21.8),\n",
    "    MD=('Moldova', 47.3, 28.7),\n",
    "    MC=('Monaco', 43.7, 7.4),\n",
    "    ME=('Montenegro', 42.3, 19.2),\n",
    "    NO=('Norway', 62.0, 7.1),\n",
    "    RO=('Romania', 45.8, 24.8),\n",
    "    RU=('Russian Federation', 55.6, 37.7),\n",
    "    ES=('Spain', 39.8, -3.4),\n",
    "    SE=('Sweden', 59.5, 16.1),\n",
    "    TR=('Turkey', 40.0, 32.8),\n",
    "    UA=('Ukraine', 49.3, 32.6),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table and Field organization\n",
    "\n",
    "Everything we do to tables and fields can be specified in the following block of configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the name of the SQL database that will hold all exported data\n",
    "\n",
    "DB_NAME = 'dariah_data'\n",
    "\n",
    "# merge source fields into one target field:\n",
    "#\n",
    "# table => target_field = (source_field1, source_field2, ...)\n",
    "\n",
    "MERGE_FIELDS = dict(\n",
    "    contrib=dict(\n",
    "        academic_entity_url=('academic_entity_url_2',),\n",
    "        contribution_url=('contribution_url_2',),\n",
    "        contact_person_mail=('contact_person_mail_2',),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# discard fields\n",
    "#\n",
    "# table => {discard_field1, discard_field2, ...}\n",
    "\n",
    "SKIP_FIELDS = dict(\n",
    "    contrib={\n",
    "        'teller',\n",
    "        'whois',\n",
    "        'help_text',\n",
    "        'help_description',\n",
    "        'total_costs_total',\n",
    "        'goldpassword',\n",
    "        'gnewpassword',\n",
    "        'gnewpassword2',\n",
    "    },\n",
    "    country={\n",
    "        'countryname',\n",
    "    },\n",
    ")\n",
    "\n",
    "# change the type of fields\n",
    "#\n",
    "# table => {field1: newtype1, field2: newtype2, ...}\n",
    "\n",
    "FIELD_TYPE_OVERRIDE = dict(\n",
    "    contrib=dict(\n",
    "        costs_total='valuta',\n",
    "        creation_date_time='datetime',\n",
    "        modification_date_time='datetime',\n",
    "        dateandtime_approval='datetime',\n",
    "        dateandtime_cioapproval='datetime',\n",
    "        dateandtime_ciozero='datetime',\n",
    "    ),\n",
    "    country=dict(\n",
    "        member_dariah='bool',\n",
    "    ),\n",
    ")\n",
    "\n",
    "# move fields from one table to a new table\n",
    "#\n",
    "# source_table => target_table => {source_field1, source_field2, ...}\n",
    "\n",
    "MOVE_FIELDS = dict(\n",
    "    contrib=dict(\n",
    "        assessment={\n",
    "            'submit',\n",
    "            'approved',\n",
    "            'vcchead_approval',\n",
    "            'vcchead_disapproval',\n",
    "            'dateandtime_approval',\n",
    "            'dateandtime_cioapproval',\n",
    "            'dateandtime_ciozero',\n",
    "            'vcc_head_decision',\n",
    "            'vcc_head_decision_vcc11',\n",
    "            'vcc_head_decision_vcc12',\n",
    "            'vcc_head_decision_vcc21',\n",
    "            'vcc_head_decision_vcc22',\n",
    "            'vcc_head_decision_vcc31',\n",
    "            'vcc_head_decision_vcc32',\n",
    "            'vcc_head_decision_vcc41',\n",
    "            'vcc_head_decision_vcc42',\n",
    "            'vcc11_name',\n",
    "            'vcc12_name',\n",
    "            'vcc21_name',\n",
    "            'vcc22_name',\n",
    "            'vcc31_name',\n",
    "            'vcc32_name',\n",
    "            'vcc41_name',\n",
    "            'vcc42_name',\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "# split field values into multiple values\n",
    "# specify a regular expression to split the value on, this can be set per field\n",
    "# refer to the splitting re-s by name, they will be defined later on\n",
    "#\n",
    "# table => {field1: splitter1, field2: splitter2, ...}\n",
    "\n",
    "SPLIT_FIELDS = dict(\n",
    "    contrib=dict(\n",
    "        disciplines_associated='generic',\n",
    "        other_keywords='generic_comma',\n",
    "        tadirah_research_activities='generic',\n",
    "        tadirah_research_objects='generic',\n",
    "        tadirah_research_techniques='generic',\n",
    "        type_of_inkind='generic',\n",
    "        vcc='generic',\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Sometimes values occur in several spellings.\n",
    "# Here we normalize it.\n",
    "# Per field we can give a list of values.\n",
    "# For every value for that field that we encounter in the data, we apply a normalize method\n",
    "# and compare it with the normalized given values. If there is a match, we replace the\n",
    "# encountered value with the given value.\n",
    "\n",
    "NORMALIZE_FIELDS = dict(\n",
    "    contrib=dict(\n",
    "        tadirah_research_activities=dict(\n",
    "            normalizer='generic',\n",
    "            values=set('''\n",
    "1. Capture\n",
    "2. Creation\n",
    "3. Enrichment\n",
    "4. Analysis\n",
    "5. Interpretation\n",
    "6. Storage\n",
    "7. Dissemination\n",
    "8. Meta-Activities\n",
    "'''.strip().split('\\n'))\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# generate a separate table for the values of some fields\n",
    "# N.B.: for fields with multiple values, this will be done automatically\n",
    "#       and for such fields a cross table will be generated as well\n",
    "#\n",
    "# table => {field1, field2, ...}\n",
    "\n",
    "VALUE_FIELDS = dict(\n",
    "    contrib={\n",
    "        'country',\n",
    "        'creator',\n",
    "        'last_modifier',\n",
    "        'other_type_of_inkind',\n",
    "        'year',\n",
    "    },\n",
    ")\n",
    "\n",
    "# sometimes the generated table of the values in a field clash with an existing table\n",
    "# with the same name which already contains those values.\n",
    "# This can be fixed by linking the values in that field to the existing table instead.\n",
    "# The fix involves mapping the identifiers in the generated table to the identifiers in the\n",
    "# existing table. This mapping is guided by a link field, i.e. the field in the existing table\n",
    "# that contains the values as found in the main table.\n",
    "# \n",
    "# table => {value_field1: link_field1, value_field2, link_field2, ...}\n",
    "\n",
    "FIX_FIELDS = dict(\n",
    "    contrib=dict(\n",
    "        country='countrycode',\n",
    "    ),\n",
    ")\n",
    "\n",
    "# sometimes we want to add fields to a table based on external information.\n",
    "# For example, we want to add certain country information that we need to\n",
    "# visualize countries on a map.\n",
    "\n",
    "ADD_FIELDS = dict(\n",
    "    country=dict(\n",
    "        link_field='countrycode',\n",
    "        fields = (\n",
    "            ('name', 'text'),\n",
    "            ('latitude', 'decimal'),\n",
    "            ('longitude', 'decimal'),\n",
    "        ),\n",
    "        data = country_extra,\n",
    "    ),\n",
    ")\n",
    "# for SQL writing: when writing insert queries, limit the number of records per statement\n",
    "\n",
    "LIMIT_ROWS = 50 # maximum number of rows to be written in one sql insert statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field types\n",
    "\n",
    "Everything we do to field types and field values can be specified in the following block of configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Source field types, including types assigned by type overriding (see FIELD_TYPE_OVERRIDE above).\n",
    "# These will be translated into appropriate SQL field types\n",
    "\n",
    "TYPES = {'bool', 'number', 'decimal', 'text', 'valuta', 'date', 'datetime'}\n",
    "\n",
    "# We inspect string values in fields, and assign varchar(size) types, where size is\n",
    "# the least power of 2 that is big enough. But there is a minimum and a maximum.\n",
    "# If the maximum is exceeded, a TEXT field (unlimited size) will be generated.\n",
    "\n",
    "MIN_M = 5       # minimum varchar size = 2**MIN_M\n",
    "MAX_M = 13      # maximum varchar size = 2**MAX_M\n",
    "\n",
    "# dates are already in ISO (date2_pattern).\n",
    "# If we encounter other dates, we could use date_pattern instead)\n",
    "# datetimes are not in iso, they will be transformed to iso.\n",
    "\n",
    "DECIMAL_PATTERN = re.compile(\n",
    "    r'^-?[0-9]+\\.?[0-9]*'\n",
    ")\n",
    "DATE_PATTERN = re.compile(\n",
    "    r'^\\s*([0-9]{2})/([0-9]{2})/([0-9]{4})$'\n",
    ")\n",
    "DATE2_PATTERN = re.compile(\n",
    "    r'^\\s*([0-9]{4})-([0-9]{2})-([0-9]{2})$'\n",
    ")\n",
    "DATETIME_PATTERN = re.compile(\n",
    "    r'^\\s*([0-9]{2})/([0-9]{2})/([0-9]{4})\\s+([0-9]{2}):([0-9]{2})(?::([0-9]{2}))?$'\n",
    ")\n",
    "\n",
    "# meaningless values will be translated into NULLs\n",
    "NULL_VALUES = {\n",
    "    'http://',\n",
    "    'https://',\n",
    "}\n",
    "\n",
    "BOOL_VALUES = {\n",
    "    True: {'Yes', 'YES', 'yes', 1, '1', True},\n",
    "    False: {'No', 'NO', 'no', 0, '0', 'NULL', False},\n",
    "}\n",
    "\n",
    "# when there is a clash between generated value tables and existing tables, the generated table's name will be\n",
    "# appended with a suffix.\n",
    "# If we have been told to fix this situation (see FIX_FIELDS above), the generated table will be discarded.\n",
    "# So, if there are table names in the result ending with this suffix, something still has to be fixed.\n",
    "\n",
    "TBF = '_tobefixed'\n",
    "\n",
    "# Here are the splitting regular expressions to split field values into multiple values\n",
    "splitters = dict(\n",
    "    generic=re.compile('[ \\t]*[\\n+][ \\t\\n]*'),          # split on newlines (with surrounding white space)\n",
    "    generic_comma=re.compile('[ \\t]*[\\n+,][ \\t\\n]*'),   # split on newlines or commas (with surrounding white space)\n",
    ")\n",
    "normalizers = dict(\n",
    "    generic=lambda x: x.lower().replace(' ',''),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config settings\n",
    "Here we specify the file names and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Locations\n",
    "\n",
    "HOME_DIR = os.path.expanduser('~').replace('\\\\', '/')\n",
    "BASE_DIR = '{}/Documents/DANS/projects/has/dacs'.format(HOME_DIR)\n",
    "FM_DIR = '{}/fm'.format(BASE_DIR)\n",
    "TEMP_DIR = '{}/tmp'.format(BASE_DIR)\n",
    "RESULT_DIR = '{}/sql'.format(BASE_DIR)\n",
    "FMNS = '{http://www.filemaker.com/fmpxmlresult}'\n",
    "ROW_RAW_FILE = '{}/row_raw_file'.format(TEMP_DIR)\n",
    "ROW_FILE = '{}/row_file'.format(TEMP_DIR)\n",
    "ROW_EXT = 'txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create directories\n",
    "\n",
    "We create temp and result directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nwarnings = 0\n",
    "\n",
    "def resetw():\n",
    "    global nwarnings\n",
    "    nwarnings = 0\n",
    "\n",
    "def info(msg):\n",
    "    sys.stdout.write('{}\\n'.format(msg))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def note(msg):\n",
    "    sys.stdout.write('NB: {}\\n'.format(msg))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def warning(msg, count=True):\n",
    "    global nwarnings\n",
    "    sys.stderr.write('{} {}: {}\\n'.format('!'*5, 'WARNING', msg))\n",
    "    sys.stderr.flush()\n",
    "    if count: nwarnings += 1\n",
    "\n",
    "def finalw():\n",
    "    if nwarnings == 0:\n",
    "        info('OK, no warnings')\n",
    "    else:\n",
    "        warning('There were {} warnings'.format(nwarnings), count=False)\n",
    "\n",
    "def check_config():\n",
    "    good = True\n",
    "    for x in [1]:\n",
    "        good = False\n",
    "        if not os.path.exists(BASE_DIR):\n",
    "            warning('BASE_DIR does not exist: {}'.format(BASE_DIR))\n",
    "            break\n",
    "        this_good = True\n",
    "        for cdir in (TEMP_DIR, RESULT_DIR):\n",
    "            this_good = False\n",
    "            if not os.path.exists(cdir):\n",
    "                try:\n",
    "                    os.makedirs(cdir)\n",
    "                except os.error as e:\n",
    "                    warning('{} could not be created.'.format(cdir))\n",
    "                    break\n",
    "            this_good = True\n",
    "        if not this_good:\n",
    "            break\n",
    "        good = True\n",
    "    if not good:\n",
    "        warning('There were configuration errors', count=False)\n",
    "    else:\n",
    "        info('Configuration OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value validation and transformation\n",
    "\n",
    "This is the code to validate and transform field values.\n",
    "They will be transformed into valid SQL values.\n",
    "String values will be surrounded by ``'``, and ``'`` inside will be escaped; newlines and tabs will be\n",
    "replaced by ``\\n`` and ``\\t``.\n",
    "\n",
    "Currency values are problematic: there is no consistency in the use of currency symbols, thousands separators, and decimal points.\n",
    "\n",
    "We assume all amounts are in euros and strip the currency symbol.\n",
    "We assume all ``.`` and ``,`` are thousands separators, unless they are followed by 1 or 2 digits only.\n",
    "In that case we treat them as decimal points. The resulting values do not contain thousands separators, and use ``.`` as decimal point.\n",
    "\n",
    "All values with a ``.`` or ``,`` inside will be shown to the user, including the resulting value, for checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def date_repl(match):\n",
    "    [d,m,y] = list(match.groups())\n",
    "    return '{}-{}-{}'.format(y,m,d)\n",
    "    \n",
    "def date2_repl(match):\n",
    "    [y,m,d] = list(match.groups())\n",
    "    return '{}-{}-{}'.format(y,m,d)\n",
    "    \n",
    "def datetime_repl(match):\n",
    "    [d,m,y,hr,mn,sc] = list(match.groups())\n",
    "    return '{}-{}-{}T{}:{}:{}'.format(y,m,d,hr,mn,sc or '00')\n",
    "    \n",
    "def sq(v_raw):\n",
    "    return \"'{}'\".format(\n",
    "        v_raw.strip().replace(\"'\",\"''\").replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "    )\n",
    "\n",
    "def bools(v_raw, i, t, fname):\n",
    "    if v_raw in BOOL_VALUES[True]: return 1\n",
    "    if v_raw in BOOL_VALUES[False]: return 0\n",
    "    warning(\n",
    "        'table `{}` field `{}` record {}: not a boolean value: \"{}\"'.format(\n",
    "            t, fname, i, v_raw\n",
    "    ))\n",
    "    return v_raw\n",
    "\n",
    "def num(v_raw, i, t, fname):\n",
    "    if type(v_raw) is int: return v_raw\n",
    "    if v_raw.isdigit(): return int(v_raw)\n",
    "    warning(\n",
    "        'table `{}` field `{}` record {}: not an integer: \"{}\"'.format(\n",
    "            t, fname, i, v_raw\n",
    "    ))\n",
    "    return v_raw\n",
    "\n",
    "def decimal(v_raw, i, t, fname):\n",
    "    if type(v_raw) is float: return v_raw\n",
    "    if v_raw.isdigit(): return float(v_raw)\n",
    "    if DECIMAL_PATTERN.match(v_raw): return float(v_raw)\n",
    "    warning(\n",
    "        'table `{}` field `{}` record {}: not an integer: \"{}\"'.format(\n",
    "            t, fname, i, v_raw\n",
    "    ))\n",
    "    return v_raw\n",
    "\n",
    "money_warnings = {}\n",
    "money_notes = {}\n",
    "\n",
    "def money(v_raw, i, t, fname):\n",
    "    note = ',' in v_raw or '.' in v_raw\n",
    "    v = v_raw.strip().lower().replace(' ','').replace('â‚¬', '').replace('euro', '').replace('\\u00a0', '')\n",
    "    for p in range(2,4): # interpret . or , as decimal point if less than 3 digits follow it\n",
    "        if len(v) >= p and v[-p] in '.,': \n",
    "            v_i = v[::-1]\n",
    "            if v_i[p-1] == ',': v_i = v_i.replace(',', 'D', 1)\n",
    "            elif v_i[p-1] == '.': v_i = v_i.replace('.', 'D', 1)\n",
    "            v = v_i[::-1]\n",
    "    v = v.replace('.','').replace(',','')\n",
    "    v = v.replace('D', '.')\n",
    "    if not v.replace('.','').isdigit():\n",
    "        if len(set(v) & set('0123456789')):\n",
    "            warning(\n",
    "                'table `{}` field `{}` record {}: not a decimal number: \"{}\" <= \"{}\"'.format(\n",
    "                    t, fname, i, v, v_raw,\n",
    "            ))\n",
    "            money_warnings.setdefault('{}:{}'.format(t, fname), {}).setdefault(v, set()).add(v_raw)\n",
    "        else:\n",
    "            v = 'NULL'\n",
    "            money_notes.setdefault('{}:{}'.format(t, fname), {}).setdefault('NULL', set()).add(v_raw)\n",
    "    elif note:\n",
    "        money_notes.setdefault('{}:{}'.format(t, fname), {}).setdefault(v, set()).add(v_raw)\n",
    "    return v\n",
    "\n",
    "def dt(v_raw, i, t, fname):\n",
    "    if not DATE2_PATTERN.match(v_raw):\n",
    "        warning(\n",
    "            'table `{}` field `{}` record {}: not a valid date: \"{}\"'.format(\n",
    "                t, fname, i, v_raw\n",
    "        ))\n",
    "        return v_raw\n",
    "    return(\"'{}'\".format(DATE2_PATTERN.sub(date2_repl, v_raw)))\n",
    "\n",
    "def dtm(v_raw, i, t, fname):\n",
    "    if not DATETIME_PATTERN.match(v_raw):\n",
    "        warning(\n",
    "            'table `{}` field `{}` record {}: not a valid date time: \"{}\"'.format(\n",
    "                t, fname, i, v_raw\n",
    "        ))\n",
    "        return v_raw\n",
    "    return(\"'{}'\".format(DATETIME_PATTERN.sub(datetime_repl, v_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the XML file\n",
    "\n",
    "Processing step: parse all XML files in the directory with exported FrameMaker files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_fm():\n",
    "    main_tables_raw = []\n",
    "    parser = etree.XMLParser(remove_blank_text=True, ns_clean=True)\n",
    "    root = {}\n",
    "    for infile in glob('{}/*.xml'.format(FM_DIR)):\n",
    "        tname = basename(splitext(infile)[0])\n",
    "        print('Parsing {}'.format(tname))\n",
    "        root[tname] = etree.parse(infile, parser).getroot()\n",
    "        main_tables_raw.append(tname)\n",
    "    return (root, main_tables_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process field specs\n",
    "\n",
    "Processing step: extract the field specifications from the XML files and apply the tweaks specified in the configuration above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_merge():\n",
    "    merge_errors = 0\n",
    "    merge_fields = {}\n",
    "\n",
    "    for t in main_tables_raw:\n",
    "        for (mfhead, mftail) in MERGE_FIELDS.get(t, {}).items():\n",
    "            for f in mftail:\n",
    "                if f in merge_fields.get(t, {}):\n",
    "                    warning(\n",
    "    'table `{}` field `{}` already merged into `{}` now to be merged into `{}`'.format(\n",
    "                        t, f, merge_fields[t][f], mfhead,\n",
    "                    ))\n",
    "                    merge_errors += 1\n",
    "                merge_fields.setdefault(t, {})[f] = mfhead\n",
    "    if merge_errors:\n",
    "        warning('There were {} merge errors'.format(merge_errors), count=False)\n",
    "    else:\n",
    "        info('Merge definitions OK')\n",
    "    return merge_fields\n",
    "\n",
    "def getfielddefs():\n",
    "    field_defs_raw = {}\n",
    "    fd_errors = 0\n",
    "    tfields = {}\n",
    "    for t in main_tables_raw:\n",
    "        fieldroots = [x for x in root[t].iter(FMNS+'METADATA')]\n",
    "        fieldroot = fieldroots[0]\n",
    "        tfields[t] = []\n",
    "        for x in fieldroot.iter(FMNS+'FIELD'):\n",
    "            fname = x.get('NAME').lower().replace(' ','_').replace(':', '_')\n",
    "            ftype = FIELD_TYPE_OVERRIDE.\\\n",
    "                get(t, {}).\\\n",
    "                get(fname, None) or x.get('TYPE').lower()\n",
    "            fmult = int(x.get('MAXREPEAT'))\n",
    "            if fname in SPLIT_FIELDS.get(t, {}): fmult += 1\n",
    "            tfields[t].append(fname)\n",
    "            field_defs_raw.setdefault(t, {})[fname] = [ftype, fmult]\n",
    "            if ftype not in TYPES:\n",
    "                warning('table `{}` field `{}` has unknown type \"{}\"'.format(\n",
    "                    t, fname, ftype,\n",
    "                ))\n",
    "                fd_errors += 1\n",
    "        if t in ADD_FIELDS:\n",
    "            added_fields = ADD_FIELDS[t]['fields']\n",
    "            for (fname, ftype) in added_fields:\n",
    "                if ftype not in TYPES:\n",
    "                    warning('table `{}` field `{}` has unknown type \"{}\"'.format(\n",
    "                        t, fname, ftype,\n",
    "                    ))\n",
    "                    fd_errors += 1\n",
    "                tfields[t].append(fname)\n",
    "                field_defs_raw.setdefault(t, {})[fname] = [ftype, 1]\n",
    "\n",
    "        info('Table {:<20}: {:>2} fields'.format(t, len(tfields[t])))\n",
    "    if fd_errors:\n",
    "        warning('There were {} field definition errors'.format(fd_errors), count=False)\n",
    "    else:\n",
    "        info('Field definitions OK')\n",
    "    return (tfields, field_defs_raw)\n",
    "\n",
    "def check_normalize():\n",
    "    for t in NORMALIZE_FIELDS:\n",
    "        for f in NORMALIZE_FIELDS[t]:\n",
    "            info = NORMALIZE_FIELDS[t][f]\n",
    "            normalizer = info['normalizer']\n",
    "            values = info['values']\n",
    "            if normalizer not in normalizers:\n",
    "                warning('Unknown normalizer in NORMALIZE_FIELDS: {}'.format(normalizer))\n",
    "                norm_function = lambda x: x\n",
    "            else:\n",
    "                norm_function = normalizers[normalizer]\n",
    "            info['value_lookup'] = dict((norm_function(val), val) for val in values)\n",
    "            info['norm_function'] = norm_function\n",
    "\n",
    "def check_merge_more():\n",
    "    merge_errors = 0\n",
    "    for t in main_tables_raw:\n",
    "        for f in merge_fields.get(t, {}):\n",
    "            if f not in field_defs_raw[t]:\n",
    "                warning(\n",
    "                    'table `{}`: cannot merge unknown field `{}`'.format(\n",
    "                    t, f,\n",
    "                ))\n",
    "                merge_errors += 1\n",
    "                continue\n",
    "            ftarget = merge_fields[t][f]\n",
    "            (ftype, fmult) = field_defs_raw[t][f]\n",
    "            if ftarget not in field_defs_raw[t]:\n",
    "                field_defs_raw[t][ftarget] = [ftype, 0]\n",
    "            (ttype, tmult) = field_defs_raw[t][ftarget]\n",
    "            if ttype != ftype:\n",
    "                warning(\n",
    "                    'table `{}` field `{}` of type \"{}\" is merged into field `{}` of other type \"{}\"'.format(\n",
    "                        t, f, ftype, ftarget, ttype,\n",
    "                ))\n",
    "                merge_errors += 1\n",
    "            field_defs_raw[t][ftarget][1] += fmult\n",
    "            del field_defs_raw[t][f]\n",
    "    if merge_errors:\n",
    "        warning('There were {} merge errors'.format(merge_errors), count=False)\n",
    "    else:\n",
    "        info('Merge OK')\n",
    "\n",
    "def do_skips():\n",
    "    fields_raw = {}\n",
    "    s_errors = 0\n",
    "    for t in main_tables_raw:\n",
    "        for f in SKIP_FIELDS.get(t, set()):\n",
    "            if f not in field_defs_raw[t]:\n",
    "                warning('table `{}`: unknown skip field `{}`'.format(t,f))\n",
    "                s_errors += 1\n",
    "            else:\n",
    "                del field_defs_raw[t][f]\n",
    "        fields_raw[t] = sorted(\n",
    "            set(field_defs_raw[t].keys()) | set(merge_fields.get(t, {}).values())\n",
    "        )\n",
    "    if s_errors:\n",
    "        warning('There were {} field skip errors'.format(s_errors), count=False)\n",
    "    else:\n",
    "        info('Field skips OK')\n",
    "\n",
    "    return fields_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "Processing step: extract the data from the XML files and transform them into a set of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getdata():\n",
    "    rows_raw = {}\n",
    "    errors = {}\n",
    "    for t in main_tables_raw:\n",
    "        if t in ADD_FIELDS:\n",
    "            xinfo = ADD_FIELDS[t]\n",
    "            link_field = xinfo['link_field']\n",
    "            extra_fields = xinfo['fields']\n",
    "            extra_field_data = xinfo['data']\n",
    "            extra_field_set = {f[0] for f in extra_fields}\n",
    "        \n",
    "        dataroots = [x for x in root[t].iter(FMNS+'RESULTSET')]\n",
    "        dataroot = dataroots[0]\n",
    "        rows_raw[t] = []\n",
    "\n",
    "        seen = set()\n",
    "        for (i, r) in enumerate(dataroot.iter(FMNS+'ROW')):\n",
    "            row = []\n",
    "            for c in r.iter(FMNS+'COL'):\n",
    "                data = [x.text for x in c.iter(FMNS+'DATA')]\n",
    "                row.append(data)\n",
    "            if t in ADD_FIELDS:\n",
    "                extra_id = None\n",
    "                for (fname, values) in zip(tfields[t], row):\n",
    "                    if fname == link_field:\n",
    "                        extra_id = ''.join(values)\n",
    "                        seen.add(extra_id)\n",
    "                for xdata in extra_field_data[extra_id]:\n",
    "                    row.append((xdata,))\n",
    "                \n",
    "            if len(row) != len(tfields[t]):\n",
    "                errors.setdefault(t, {}).setdefault('Number of fields', []).append(i)\n",
    "            rows_raw[t].append(row)\n",
    "        if t in ADD_FIELDS:\n",
    "            for i in extra_field_data:\n",
    "                if i in seen: continue\n",
    "                row = []\n",
    "                j = 0\n",
    "                for f in tfields[t]:\n",
    "                    if f == link_field:\n",
    "                        row.append((i,))\n",
    "                    elif f not in extra_field_set:\n",
    "                        row.append(('NULL',))\n",
    "                for xdata in extra_field_data[i]:\n",
    "                    row.append((xdata,))\n",
    "                rows_raw[t].append(row)\n",
    "\n",
    "        rf = open('{}_{}.{}'.format(ROW_RAW_FILE, t, ROW_EXT), 'w')\n",
    "        for row in rows_raw[t]:\n",
    "            for (fname, values) in zip(tfields[t], row):\n",
    "                rf.write('@{:>30} = {}\\n'.format(\n",
    "                    fname,\n",
    "                    ' | '.join('{}'.format(v) for v in values),\n",
    "                ))\n",
    "            rf.write('{}\\n'.format('='*100))\n",
    "        rf.close()\n",
    "        info('Table {:<20}: {:>4} rows read'.format(t, len(rows_raw[t])))\n",
    "    if errors:\n",
    "        for t in sorted(errors):\n",
    "            for k in sorted(errors[t]):\n",
    "                warning('table {:<20}: {:<20}: {}'.format(t, k, ','.join(str(i) for i in errors[t][k])))\n",
    "    else:\n",
    "        info('Data import OK')\n",
    "    return rows_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the values\n",
    "\n",
    "Processing step:\n",
    "* various non-informational values will be converted to NULL;\n",
    "* values will be thinned: \n",
    "  identical values will be reduced to one copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transformrows():\n",
    "    rows = {}\n",
    "    money_warnings.clear()\n",
    "    warnings = {}\n",
    "    for t in main_tables_raw:\n",
    "        for (i, row_raw) in enumerate(rows_raw.get(t, [])):\n",
    "            values = {}\n",
    "            for (fname, values_raw) in zip(tfields[t], row_raw):\n",
    "                if fname in SKIP_FIELDS.get(t, set()): continue\n",
    "                sep = SPLIT_FIELDS.get(t, {}).get(fname, None)\n",
    "                if sep != None:\n",
    "                    values_raw = sorted(reduce(\n",
    "                        set.union, \n",
    "                        [set(splitters[sep].split(v)) for v in values_raw if v != None], \n",
    "                        set(),\n",
    "                    ))\n",
    "                    if '' in values_raw: values_raw.remove('')\n",
    "                norm_info = NORMALIZE_FIELDS.get(t, {}).get(fname, None)\n",
    "                if norm_info != None:\n",
    "                    vlookup = norm_info['value_lookup']\n",
    "                    values_norm = []\n",
    "                    for v in values_raw:\n",
    "                        vn = norm_info['norm_function'](v)\n",
    "                        if vn not in vlookup:\n",
    "                            warnings.setdefault(t, {}).setdefault(fname, {}).setdefault(v, set()).add(i)\n",
    "                            vn = v\n",
    "                        values_norm.append(vlookup[vn])\n",
    "                    values_raw = values_norm\n",
    "                ftarget = merge_fields.get(t, {}).get(fname, fname)\n",
    "                (ftype, fmult) = field_defs_raw[t][ftarget]\n",
    "                valset = set()\n",
    "                for v_raw in values_raw:\n",
    "                    if v_raw == None or v_raw in NULL_VALUES: v = 'NULL'\n",
    "                    elif ftype == 'text': v = sq(v_raw)\n",
    "                    elif ftype == 'bool': v = bools(v_raw, i, t, fname)\n",
    "                    elif ftype == 'number': v = num(v_raw, i, t, fname)\n",
    "                    elif ftype == 'decimal': v = decimal(v_raw, i, t, fname)\n",
    "                    elif ftype == 'valuta': v = money(v_raw, i, t, fname)\n",
    "                    elif ftype == 'date': v = dt(v_raw, i, t, fname)\n",
    "                    elif ftype == 'datetime': v = dtm(v_raw, i, t, fname)\n",
    "                    else: v = v_raw\n",
    "                    valset.add(v)\n",
    "                if fmult > 1: valset.discard('NULL')\n",
    "                these_values = values.setdefault(ftarget, set())\n",
    "                these_values |= valset\n",
    "            rows.setdefault(t, []).append(values)\n",
    "        info('Table `{}`: {:>5} rows checked'.format(t, len(rows[t])))\n",
    "\n",
    "        rf = open('{}_{}.{}'.format(ROW_FILE, t, ROW_EXT), 'w')\n",
    "        for row in rows[t]:\n",
    "            for (fname, values) in sorted(row.items()):\n",
    "                rf.write('@{:>30} = {}\\n'.format(\n",
    "                    fname,\n",
    "                    ' | '.join('{}'.format(v) for v in sorted(values)),\n",
    "                ))\n",
    "            rf.write('{}\\n'.format('='*100))\n",
    "        rf.close()\n",
    "    if warnings:\n",
    "        for t in warnings:\n",
    "            for f in warnings[t]:\n",
    "                for v in warnings[t][f]:\n",
    "                    warning('table `{}`, field `{}`: no normal value for \"{}\" in {}'.format(\n",
    "                        t, f, v, ','.join(str(i) for i in warnings[t][f][v]),\n",
    "                    ))\n",
    "    if money_notes:\n",
    "        for tf in sorted(money_notes):\n",
    "            for v in sorted(money_notes[tf]):\n",
    "                note('{} \"{}\" <= {}'.format(\n",
    "                    tf, v,\n",
    "                    ' | '.join(money_notes[tf][v]),\n",
    "            ))\n",
    "\n",
    "    if money_warnings:\n",
    "        for tf in sorted(money_warnings):\n",
    "            for v in sorted(money_warnings[tf]):\n",
    "                warning('{} \"{}\" <= {}'.format(\n",
    "                    tf, v,\n",
    "                    ' | '.join(money_warnings[tf][v]),\n",
    "            ))\n",
    "    else:\n",
    "        info('Money OK')\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn the data into a dict\n",
    "\n",
    "Procssing step:\n",
    "* represent the data as column dictionaries:\n",
    "  * keys are the field names,\n",
    "  * values are lists of field values, in the same order as the original rows\n",
    "  \n",
    "The column dicts are handy to perform field and table organization.\n",
    "Later we will reconvert them to rows again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pivot():\n",
    "    field_data_raw = {}\n",
    "    for t in main_tables_raw:\n",
    "        for row in rows[t]:\n",
    "            for (fname, values) in sorted(row.items()):\n",
    "                field_data_raw.setdefault(t, {}).setdefault(fname, []).append(values)\n",
    "        info('Table `{}`: {:<5} records and {:<2} fields pivoted'.format(\n",
    "            t, len(rows[t]), len(field_data_raw[t]),\n",
    "        ))\n",
    "\n",
    "    # check\n",
    "    good = True\n",
    "    for t in field_data_raw:\n",
    "        for f in field_data_raw[t]:\n",
    "            if len(field_data_raw[t][f]) != len(rows[t]):        \n",
    "                warning(\n",
    "        'table `{}`, field `{}`: wrong number of records: {} instead of {}'.format(\n",
    "                    t, f, len(field_data_raw[t][f]), len(rows[t]),\n",
    "                ))\n",
    "                good = False\n",
    "    if good:\n",
    "        info('Pivot OK')\n",
    "    else:\n",
    "        warning('There were errors', count=False)\n",
    "    return field_data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move fields\n",
    "\n",
    "Processing step:\n",
    "* move fields from one table to another, according to the specification in the config block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move_fields():\n",
    "    errors = 0\n",
    "    main_tables = deepcopy(main_tables_raw)\n",
    "    fields = deepcopy(fields_raw)\n",
    "    field_defs = deepcopy(field_defs_raw)\n",
    "    field_data = deepcopy(field_data_raw)\n",
    "    for t in MOVE_FIELDS:\n",
    "        if t not in field_data:\n",
    "            warning('move fields from table `{}`: this table does not exist'.format(\n",
    "                t,\n",
    "            ))\n",
    "            errors += 1\n",
    "            continue\n",
    "        for t_new in MOVE_FIELDS[t]:\n",
    "            main_tables.append(t_new)\n",
    "            nid = '{}_id'.format(t)\n",
    "            field_data.setdefault(t_new, {})[nid] = [{i} for i in range(len(rows[t]))]\n",
    "            field_defs.setdefault(t_new, {})[nid] = ((t, 'id'), 1)\n",
    "            move_fields = set(MOVE_FIELDS[t][t_new])\n",
    "            for f in sorted(move_fields):\n",
    "                if f not in field_data[t]:\n",
    "                    warning(\n",
    "            'table `{}`: move field `{}` to `{}`: this field does not exist'.format(\n",
    "                        t, f, t_new,\n",
    "                    ))\n",
    "                    errors += 1\n",
    "                    move_fields.remove(f)\n",
    "                    continue\n",
    "                field_data.setdefault(t_new, {})[f] = field_data[t][f]\n",
    "                del field_data[t][f]\n",
    "                field_defs.setdefault(t_new, {})[f] = field_defs[t][f]\n",
    "                del field_defs[t][f]\n",
    "            fields[t] = sorted(set(fields[t]) - move_fields)\n",
    "            fields[t_new] = [nid]+sorted(move_fields)\n",
    "            info('moved fields\\n\\t`{}`\\nfrom `{}` to `{}`'.format(\n",
    "                '`\\n\\t`'.join(sorted(move_fields)), t, t_new, \n",
    "            ))\n",
    "            \n",
    "    if errors:\n",
    "        warning('There were {} errors'.format(errors), count=False)\n",
    "    else:\n",
    "        info('Move fields OK')\n",
    "    return (main_tables, fields, field_defs, field_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract related tables\n",
    "\n",
    "Processing step:\n",
    "* generate value tables for specified fields;\n",
    "* generate value tables and cross tables for fields with multiple values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(t, fname, maindata, relvalues, relindex, reltables, relxtables, relfieldindex):\n",
    "    fname_alt = fname\n",
    "    if fname in main_tables:\n",
    "        link_field = FIX_FIELDS.get(t, {}).get(fname, None)\n",
    "        if link_field:\n",
    "            note('table `{}`: value field `{}` will be linked to `{}:{}'.format(\n",
    "                t, fname, fname, link_field,\n",
    "        ))\n",
    "        else:\n",
    "            warning('table `{}`: value field `{}` is already a table!'.format(t, fname))\n",
    "        fname_alt = '{}{}'.format(fname, TBF)\n",
    "    is_single = field_defs[t][fname][1] == 1 # single value or multiple values\n",
    "    error = False\n",
    "    if fname in relfieldindex:\n",
    "        warning(\n",
    "    'related table `{}` extracted from `{}` and earlier from `{}`'.format(\n",
    "            fname, t, relfieldindex[fname],\n",
    "        ))\n",
    "        error = True\n",
    "    relfieldindex[fname] = t\n",
    "    for (i, values) in enumerate(field_data[t][fname]):\n",
    "        for value in values:\n",
    "            vid = relvalues.setdefault(fname, {}).get(value, None)\n",
    "            if vid == None:\n",
    "                relindex[fname] += 1\n",
    "                vid = relindex[fname]\n",
    "                reltables.setdefault(fname_alt, []).append((vid, value))\n",
    "            relvalues[fname][value] = vid\n",
    "            if is_single:\n",
    "                maindata[t][fname][i] = {vid}\n",
    "            else:\n",
    "                relxtables.setdefault(fname_alt, []).append((i, vid))\n",
    "    if not is_single: del maindata[t][fname]\n",
    "    return error\n",
    "\n",
    "def transform_data():\n",
    "    maindata = deepcopy(field_data)\n",
    "    relvalues = {} # dicts\n",
    "    relindex = collections.Counter()\n",
    "    reltables = {} # lists\n",
    "    relxtables = {} # lists\n",
    "    relfieldindex = {}\n",
    "    errors = 0\n",
    "    for t in main_tables:\n",
    "        field_list =\\\n",
    "            VALUE_FIELDS.get(t, set()) |\\\n",
    "            {f for f in fields[t] if field_defs[t][f][1] > 1}\n",
    "        for fname in field_list:\n",
    "            if fname not in field_defs[t]:\n",
    "                warning('table `{}`: wrong field {}'.format(t, fname))\n",
    "                errors += 1\n",
    "                continue\n",
    "            error = extract(\n",
    "                t, fname, maindata, \n",
    "                relvalues, relindex, reltables, relxtables, relfieldindex,\n",
    "            )\n",
    "            if error: errors +=1\n",
    "    if errors:\n",
    "        warning('There were {} extraction errors'.format(errors), count=False)\n",
    "    else:\n",
    "        info('Extraction OK')\n",
    "    return (maindata, reltables, relxtables, relvalues, relfieldindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix fields\n",
    "\n",
    "Processing step:\n",
    "* fix clashes between generated value tables and existing, richer value tables.\n",
    "\n",
    "This is done by computing a mapping between the ids in the two tables, based on corresponding values in a certain field of the existing table. These values are the ones that occur in the field of the main table for which a value table has been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getmapping(main_t, main_f):\n",
    "    rel_t = '{}{}'.format(main_t, TBF)\n",
    "    rel_f = main_t\n",
    "    main_codes = maindata[main_t][main_f]\n",
    "    rel_codes = reltables[rel_t]\n",
    "    main_index = dict((list(c)[0],i) for (i,c) in enumerate(main_codes))\n",
    "    rel_index = dict((i,c) for (i,c) in rel_codes)\n",
    "    return dict((i, main_index[rel_index[i]]) for i in rel_index)\n",
    "\n",
    "def fix(t, main_t, main_f):\n",
    "    mapping = getmapping(main_t, main_f)\n",
    "    rel_t = '{}{}'.format(main_t, TBF)\n",
    "    if rel_t in reltables: del reltables[rel_t]\n",
    "\n",
    "    new_maindata = [{mapping[list(x)[0]]} for x in maindata[t][main_t]]\n",
    "    maindata[t][main_t] = new_maindata\n",
    "    main_tables.remove(main_t)\n",
    "    main_tables.insert(0, main_t)\n",
    "\n",
    "def fix_them():\n",
    "    for t in FIX_FIELDS:\n",
    "        for main_t in FIX_FIELDS[t]:\n",
    "            link_field = FIX_FIELDS[t][main_t]\n",
    "            note('linking `{}:{}` to table `{}` on `{}`'.format(\n",
    "                t, main_t, main_t, link_field,\n",
    "            ))\n",
    "            fix(t, main_t, link_field)\n",
    "    info('Fixing OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write sql to file\n",
    "Processing step:\n",
    "\n",
    "Write the SQL files.\n",
    "\n",
    "Two files are written:\n",
    "* create statements (this is the data model)\n",
    "* insert statements (here is all the data).\n",
    "\n",
    "The files will drop the database if it exists, and (re)create all data.\n",
    "\n",
    "Care has to be taken that the tables are created before other tables that depend on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getsize(source, fname):\n",
    "    values = set()\n",
    "    for vals in source: values |= set(vals)\n",
    "    maxlen = max({len(x) for x in values if x != 'NULL'}, default=0)\n",
    "    result = 0\n",
    "    for m in range(MIN_M, MAX_M+1):\n",
    "        if maxlen <= 2**m:\n",
    "            result = m\n",
    "            break\n",
    "    if maxlen > 2**MAX_M:\n",
    "        note(\n",
    "            'Field `{}`: value with length {} gets type TEXT'.format(\n",
    "                fname, maxlen, 2**MAX_M,\n",
    "        ))\n",
    "        return False\n",
    "    return 2**m\n",
    "\n",
    "def getdef(source, t, fname, newfname, warn_mult=True):\n",
    "    (ft, fmult) = field_defs[t][fname]\n",
    "    if warn_mult and fmult > 1:\n",
    "        warning(\n",
    "            'skipping field `{}` because it contains multiple values'.format(\n",
    "                fname,\n",
    "        ))\n",
    "        return None\n",
    "    if type(ft) is tuple:\n",
    "        (ftable, ffield) = ft\n",
    "        ftype = 'int'\n",
    "        fsize = '(4)'\n",
    "        fext = ',\\n\\tforeign key ({}) references {}({})'.format(fname, ftable, ffield)\n",
    "    elif ft == 'bool':\n",
    "        ftype = 'tinyint'\n",
    "        fsize = '(1)'\n",
    "        fext = ''\n",
    "    elif ft == 'number':\n",
    "        ftype = 'int'\n",
    "        fsize = '(4)'\n",
    "        fext = ''\n",
    "    elif ft == 'decimal':\n",
    "        ftype = 'decimal'\n",
    "        fsize = '(10,2)'\n",
    "        fext = ''\n",
    "    elif ft == 'text':\n",
    "        ftype = 'varchar'\n",
    "        fsize_raw = getsize(source, fname)\n",
    "        if not fsize_raw:\n",
    "            ftype = 'text'\n",
    "            fsize = ''\n",
    "        else:\n",
    "            fsize = '({})'.format(fsize_raw)\n",
    "        fext = 'character set utf8'\n",
    "    elif ft == 'valuta':\n",
    "        ftype = 'decimal'\n",
    "        fsize = '(10,2)'\n",
    "        fext = ''\n",
    "    elif ft == 'date':\n",
    "        ftype = 'datetime'\n",
    "        fsize = ''\n",
    "        fext = ''\n",
    "    elif ft == 'datetime':\n",
    "        ftype = 'datetime'\n",
    "        fsize = ''\n",
    "        fext = ''\n",
    "    else:\n",
    "        warning('skipping field `{}` because it has unknown type `{}`'.format(\n",
    "            fname, ft,\n",
    "        ))\n",
    "        return None\n",
    "    return '{} {}{} {}'.format(newfname, ftype, fsize, fext)\n",
    "\n",
    "def getrdef(fname):\n",
    "    return '''{fn}_id int(4),\n",
    "    foreign key ({fn}_id) references {fn}(id)'''.format(fn=fname)\n",
    "\n",
    "def sql_data(df, tname, flist, rows):\n",
    "    head = 'insert into {} ({}) values'.format(tname, ','.join(flist))\n",
    "    for (i, row) in enumerate(rows):\n",
    "        if i % LIMIT_ROWS == 0:\n",
    "            if i > 0: df.write(';')\n",
    "            df.write('\\n')\n",
    "            df.write('select \"table {} row {}\" as \" \";\\n'.format(tname, i))\n",
    "            df.write(head)\n",
    "            sep = ''\n",
    "        df.write('\\n{}\\t'.format(sep))\n",
    "        sep = ','\n",
    "        df.write('({})'.format(','.join(str(x) for x in row)))\n",
    "    df.write(';\\n')\n",
    "        \n",
    "def print_maintables(maindata, reltables, cf, df):\n",
    "    errors = 0\n",
    "    for t in main_tables:\n",
    "        fdefs = ['id int(4) primary key']\n",
    "        flist = sorted(maindata[t])\n",
    "        fnewlist = []\n",
    "        for fname in flist:\n",
    "            if fname in reltables or fname in FIX_FIELDS.get(t, {}):\n",
    "                fdef = getrdef(fname)\n",
    "                fnewname = '{}_id'.format(fname)\n",
    "            else:\n",
    "                fdef = getdef(field_data[t][fname], t, fname, fname)\n",
    "                if fdef == None:\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                fnewname = fname\n",
    "            fdefs.append(fdef)\n",
    "            fnewlist.append(fnewname)\n",
    "        cf.write('''\n",
    "create table {} (\n",
    "    {}\n",
    ");\n",
    "    '''.format(t, ',\\n\\t'.join(fdefs)))\n",
    "        maintable_raw = zip(*(maindata[t][f] for f in flist))\n",
    "        maintable = [\n",
    "            [i]+[sorted(vals)[0] for vals in row] for (i, row) in enumerate(maintable_raw)\n",
    "        ]\n",
    "        sql_data(df, t, ['id'] + fnewlist, maintable)\n",
    "    return errors\n",
    "\n",
    "def print_reltables(reltables, relvalues, cf, df):\n",
    "    errors = 0\n",
    "    for tname_alt in sorted(reltables):\n",
    "        tname = tname_alt\n",
    "        pos = tname_alt.rfind(TBF)\n",
    "        if pos > 0: tname = tname_alt[0:pos]\n",
    "        fdefs = ['id int(4) primary key']\n",
    "        fdef = getdef(\n",
    "            [relvalues[tname].keys()], \n",
    "            relfieldindex[tname], \n",
    "            tname, 'val', warn_mult=False,\n",
    "        )\n",
    "        if fdef == None:\n",
    "            errors += 1\n",
    "            continue            \n",
    "        fdefs.append(fdef)\n",
    "        cf.write('''\n",
    "create table {} (\n",
    "    {}\n",
    ");\n",
    "'''.format(tname_alt, ',\\n\\t'.join(fdefs)))\n",
    "        sql_data(df, tname_alt, ['id', 'val'], reltables[tname_alt])\n",
    "    return errors\n",
    "\n",
    "def print_relxtables(relxtables, cf, df):\n",
    "    errors = 0\n",
    "    for tname_alt in sorted(relxtables):\n",
    "        tname = tname_alt\n",
    "        pos = tname_alt.rfind(TBF)\n",
    "        if pos > 0: tname = tname_alt[0:pos]\n",
    "        t = relfieldindex[tname]\n",
    "        tname_rep = '{}_{}'.format(t, tname_alt)\n",
    "        main_id = '{}_id'.format(t)\n",
    "        val_id = '{}_id'.format(tname)\n",
    "        fdefs = '''\n",
    "    {mi} int(4),\n",
    "    {vi} int(4),\n",
    "    foreign key ({mi}) references {mt}(id),\n",
    "    foreign key ({vi}) references {tn}(id)\n",
    "'''.format(mt=t, mi=main_id, tn=tname, vi=val_id)\n",
    "        cf.write('''\n",
    "create table {} ({});\n",
    "'''.format(tname_rep, fdefs))\n",
    "        sql_data(df, tname_rep, [main_id, val_id], relxtables[tname_alt])\n",
    "    return errors\n",
    "\n",
    "\n",
    "def sql_export():\n",
    "    errors = 0\n",
    "    cf = open('{}/create.sql'.format(RESULT_DIR), 'w')\n",
    "    df = open('{}/data.sql'.format(RESULT_DIR), 'w')\n",
    "    df.write('''\n",
    "select \"FILL TABLES OF DATABASE {db}\" as \" \";\n",
    "\n",
    "use {db};\n",
    "\n",
    "'''.format(db=DB_NAME))\n",
    "\n",
    "    cf.write('''\n",
    "select \"CREATE DATABASE {db} AND TABLES\" as \" \";\n",
    "\n",
    "drop database if exists {db};\n",
    "create database {db} character set utf8;\n",
    "use {db};\n",
    "\n",
    "'''.format(db=DB_NAME))\n",
    "    cf.write('/* value tables */\\n')\n",
    "    df.write('\\n/* value tables */\\n')\n",
    "    errors += print_reltables(reltables, relvalues, cf, df)\n",
    "\n",
    "    cf.write('/* main tables */\\n')\n",
    "    df.write('\\n/* main tables */\\n')\n",
    "    errors += print_maintables(maindata, reltables, cf, df)\n",
    "\n",
    "    cf.write('/* cross tables */\\n')\n",
    "    df.write('\\n/* cross tables */\\n')\n",
    "    errors += print_relxtables(relxtables, cf, df)\n",
    "\n",
    "    cf.close()\n",
    "    df.close()\n",
    "    \n",
    "    if errors:\n",
    "        warning('There were {} errors'.format(errors), count=False)\n",
    "    else:\n",
    "        info('SQL OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert into MongoDB\n",
    "\n",
    "We have defined a fully normalized model to get our data into SQL.\n",
    "However, we also want to insert our data into MongoDB.\n",
    "To that end, we will denormalize some data and model the data as documents (JSON structures) with a mix of embedding and references.\n",
    "\n",
    "Even if we choose embedding for certain related information, there are still choices. For example, the TADIRAH categories can be embedded in contribution documents as arrays of string values. But maybe those categories will get multilingual labels. Also, at the client side we want to do filtering on these categories, and that is much more cleanly programmed if we have ids for those categories and if contributions refer to those ids.\n",
    "On the other hand, every contribution will have just a handful of categories at most, so we do not make an independent table of categories. Instead, we embed categories plus their id into the contrib document.\n",
    "This reasoning follows the [rules of thumb of MongoDB modeling part 1](http://blog.mongodb.org/post/87200945828/6-rules-of-thumb-for-mongodb-schema-design-part-1).\n",
    "\n",
    "We create a database with collections with documents as a Python dict, and then talk to MongoDB directly through its Python client. In this way we import all the data to MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mconv = collections.Counter()\n",
    "\n",
    "def mj(t, f, rid, val, skipref):\n",
    "    if f.endswith('_id') or (skipref and relfieldindex.get(f) == t): \n",
    "        ftype = 'ref'\n",
    "    else:\n",
    "        ftype = field_defs[t][f][0]\n",
    "    mconv[ftype] += 1\n",
    "\n",
    "    if val == 'NULL': v = None\n",
    "    elif ftype == 'text': v = val[1:-1].replace(\"''\", \"\\\\'\").replace('\"', '\\\\\"')\n",
    "    elif ftype == 'bool': v = True if val == 1 else False\n",
    "    elif ftype == 'number': v = int(val)\n",
    "    elif ftype == 'decimal': v = float(val)\n",
    "    elif ftype == 'valuta': v = float(val)\n",
    "    elif ftype == 'date': v = datetime(*map(int, re.split('[:T-]', val[1:-1])))\n",
    "    elif ftype == 'datetime': v = datetime(*map(int, re.split('[:T-]', val[1:-1])))\n",
    "    else: v = val\n",
    "    if rid != None: v = {'_id': '{}'.format(rid), 'value': v}\n",
    "    return v\n",
    "\n",
    "def mongo_transform():\n",
    "    \n",
    "    mongo_db = {}\n",
    "\n",
    "    errors = 0\n",
    "    embed = {}\n",
    "    for r in reltables.keys():\n",
    "        t = relfieldindex[r]\n",
    "        embed.setdefault(t, set()).add(r) # all value fields wil be embedded again, \n",
    "                                          # but as subdocs with ids!\n",
    "\n",
    "    for t in main_tables:\n",
    "        lookup = {}\n",
    "        fdefs = ['_id']\n",
    "        flist = sorted(maindata[t])\n",
    "        fnewlist = []\n",
    "        embed_info = embed.get(t, set())\n",
    "        embed_fields = set()\n",
    "        ref_fields = set()\n",
    "        for fname in flist:\n",
    "            fncomps = fname.rsplit('_', 1)\n",
    "            fnamex = fncomps[0]\n",
    "            if fname in reltables or fname in FIX_FIELDS.get(t, {}):\n",
    "                fdef = getrdef(fname)\n",
    "                if fname in embed_info:\n",
    "                    fnewname = fname\n",
    "                    embed_fields.add(fname)\n",
    "                else:\n",
    "                    ref_fields.add(fname)\n",
    "                    fnewname = '{}_id'.format(fname)\n",
    "            elif fname.endswith('_id') and fnamex in main_tables:\n",
    "                    ref_fields.add(fnamex)\n",
    "                    fnewname = fname\n",
    "            else:\n",
    "                fdef = getdef(field_data[t][fname], t, fname, fname)\n",
    "                if fdef == None:\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                fnewname = fname\n",
    "            fdefs.append(fdef)\n",
    "            fnewlist.append(fnewname)\n",
    "        for ef in embed.get(t, set()):\n",
    "            lookup[ef] = dict(row for row in reltables[ef])\n",
    "        maintable_raw = zip(*(maindata[t][f] for f in flist))\n",
    "        mongo_db[t] = []\n",
    "        for (i, row) in enumerate(maintable_raw):\n",
    "            doc = dict((fname, mj(t, fname, None, sorted(fvals)[0], True)) for (fname, fvals) in zip(fnewlist, row))\n",
    "            doc['_id'] = '{}'.format(i)\n",
    "            for ef in embed_fields:\n",
    "                rid = doc[ef]\n",
    "                rval = lookup[ef][rid]\n",
    "                doc[ef] = mj(t, ef, rid, rval, False)\n",
    "            for rf in ref_fields:\n",
    "                idfname = '{}_id'.format(rf)\n",
    "                rid = doc[idfname]\n",
    "                doc[idfname] = '{}'.format(rid)\n",
    "            mongo_db[t].append(doc)\n",
    "        for r in relxtables:\n",
    "            if relfieldindex[r] == t:\n",
    "                if r in embed.get(t, set()):\n",
    "                    rfname = r\n",
    "                    byref = False\n",
    "                else:\n",
    "                    rfname = '{}_id'.format(r)\n",
    "                    byref = True\n",
    "                for (main_id, rid) in relxtables[r]:\n",
    "                    rval = '{}'.format(rid) if byref else mj(t, r, rid, lookup[r][rid], False)\n",
    "                    mongo_db[t][main_id].setdefault(rfname, []).append(rval)\n",
    "    if errors:\n",
    "        warning('There were {} errors'.format(errors), count=False)\n",
    "    else:\n",
    "        info('MONGO_TRANSFORM OK')\n",
    "    return mongo_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to embed a few country fields in the main contrib document.\n",
    "We just hack it.\n",
    "\n",
    "We also replace the original country id by the country code.\n",
    "The table contrib is the only one that links to countries.\n",
    "\n",
    "We also want to rename a few fields (from under_score convention to camelCase convention, for more harmony with Meteor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MONGO_FIELDS = dict(\n",
    "    vcchead=dict(\n",
    "        vcc_head_name='vccHeadName',\n",
    "    ),\n",
    "    remark=dict(\n",
    "        remark_date_time='dateRemark',\n",
    "    ),\n",
    "    workinggroup=dict(\n",
    "        working_group_name='name',\n",
    "        main_vcc='vccMain',\n",
    "        liaison_vcc='vccLiaison',\n",
    "        potential_participants='participantsPotential',\n",
    "        general_contribution_to_dariah_infrastructure='infraContribution',\n",
    "        relation_to_dariah_objectives='dariahObjectives',\n",
    "        in_kind_provisions='contributionsInkind',\n",
    "        additional_funding='fundingAdditional',\n",
    "        future_location_of_the_service='locationServiceFuture',\n",
    "        date_proposal='dateProposed',\n",
    "        date_approval_jrc='dateApprovalJrc',\n",
    "        date_approval_smt='dateApprovalSMT',\n",
    "        extra_info='extraInfo',\n",
    "        theme__access='themeAccess',\n",
    "        theme__sustainability='themeSustainability',\n",
    "        theme__impact='themeImpact',\n",
    "        theme__interoperability='themeInteroperability',\n",
    "        theme__training='themeTraining',\n",
    "    ),\n",
    "    contrib=dict(\n",
    "        creation_date_time='dateCreated',\n",
    "        modification_date_time='dateModified',\n",
    "        last_modifier='modifiedBy',\n",
    "        costs_total='costTotal',\n",
    "        total_costs_total='costTotalTotal',\n",
    "        costs_descritpion='costDescription',\n",
    "        description_of_contribution='description',\n",
    "        ikid_base='ikidBase',\n",
    "        academic_entity_url='urlAcademic',\n",
    "        contribution_url='urlContribution',\n",
    "        contact_person_mail='contactPersonEmail',\n",
    "        contact_person_name='contactPersonName',\n",
    "        type_of_inkind='typeContribution',\n",
    "        other_type_of_inkind='typeContributionOther',\n",
    "        disciplines_associated='disciplines',\n",
    "        tadirah_research_activities='tadirahActivities',\n",
    "        tadirah_research_objects='tadirahObjects',\n",
    "        tadirah_research_techniques='tadirahTechniques',\n",
    "        other_keywords='keywords',\n",
    "        find_type='findType',\n",
    "        find_country_id='findCountry',\n",
    "        message_allert='alert',\n",
    "\n",
    "    ),\n",
    "    assessment=dict(\n",
    "        dateandtime_approval='dateApproval',\n",
    "        dateandtime_cioapproval='dateApprovalCIO',\n",
    "        dateandtime_ciozero='dateZeroCIO',\n",
    "        vcchead_approval='vccHeadApproval',\n",
    "        vcchead_disapproval='vccHeadDisapproval',\n",
    "        vcchead_decision='vccHeadDecision',\n",
    "        vcc_head_decision_vcc11='vccHeadDecision11',\n",
    "        vcc_head_decision_vcc12='vccHeadDecision12',\n",
    "        vcc_head_decision_vcc21='vccHeadDecision21',\n",
    "        vcc_head_decision_vcc22='vccHeadDecision22',\n",
    "        vcc_head_decision_vcc31='vccHeadDecision31',\n",
    "        vcc_head_decision_vcc32='vccHeadDecision32',\n",
    "        vcc_head_decision_vcc41='vccHeadDecision41',\n",
    "        vcc_head_decision_vcc42='vccHeadDecision42',\n",
    "        vcc11_name='vccName11',\n",
    "        vcc12_name='vccName12',\n",
    "        vcc21_name='vccName21',\n",
    "        vcc22_name='vccName22',\n",
    "        vcc31_name='vccName31',\n",
    "        vcc32_name='vccName32',\n",
    "        vcc41_name='vccName41',\n",
    "        vcc42_name='vccName42',\n",
    "    ),\n",
    "    country=dict(\n",
    "        member_dariah='inDARIAH',\n",
    "    ),\n",
    "    help=dict(\n",
    "        help_description='description',\n",
    "        help_text='text',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mongo_rename():\n",
    "    for c in MONGO_FIELDS:\n",
    "        fields = MONGO_FIELDS[c]\n",
    "        for d in mongo_db[c]:\n",
    "            for (of, nf) in fields.items():\n",
    "                if of in d:\n",
    "                    d[nf] = d[of]\n",
    "                    del d[of]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mongo_dedup():\n",
    "    countries = {}\n",
    "    for c in mongo_db['country']:\n",
    "        countries[c['_id']] = c\n",
    "        c['_id'] = c['countrycode']\n",
    "        del c['countrycode']\n",
    "    for c in mongo_db['contrib']:\n",
    "        cn_id = c['country_id']\n",
    "        cn_dt = countries[cn_id]\n",
    "        c['country'] = [{'_id': cn_dt['_id'], 'name': cn_dt['name']}]\n",
    "        del c['country_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def import_mongo():\n",
    "    client = MongoClient()\n",
    "    client.drop_database('dariah')\n",
    "    db = client.dariah\n",
    "    for t in main_tables:\n",
    "        info(t)\n",
    "        db[t].insert_many(mongo_db[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import the bson dump in another mongodb installation, use the commandline to dump the dariah database here\n",
    "\n",
    "    mongodump -d dariah -o dariah\n",
    "\n",
    "and to import it elsewhere.\n",
    "\n",
    "    mongorestore --drop -d dariah dariah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the whole pipeline\n",
    "\n",
    "All processing steps are chained together here.\n",
    "\n",
    "There is detailed progress indication. \n",
    "If irregularities are encountered, they will show up very clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "!!!!! WARNING: table `assess` field `date_approved`: strange value \"2017-00-03\"\n"
     ]
    }
   ],
   "source": [
    "# example of a warning message\n",
    "warning('table `{}` field `{}`: strange value \"{}\"'.format('assess', 'date_approved', '2017-00-03'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================BEGIN PROCESSING================================\n",
      "==================================CHECK CONFIG==================================\n",
      "Configuration OK\n",
      "====================================READ FM=====================================\n",
      "Parsing contrib\n",
      "Parsing country\n",
      "Parsing help\n",
      "Parsing remark\n",
      "Parsing vcchead\n",
      "Parsing workinggroup\n",
      "================================MERGE pre CHECK=================================\n",
      "Merge definitions OK\n",
      "===============================FIELD DEFINITIONS================================\n",
      "Table contrib             : 63 fields\n",
      "Table country             :  6 fields\n",
      "Table help                :  2 fields\n",
      "Table remark              :  5 fields\n",
      "Table vcchead             :  2 fields\n",
      "Table workinggroup        : 27 fields\n",
      "Field definitions OK\n",
      "===========================FIELD VALUE NORMALIZATION============================\n",
      "================================MERGE post CHECK================================\n",
      "Merge OK\n",
      "==================================SKIP FIELDS===================================\n",
      "Field skips OK\n",
      "===================================READ DATA====================================\n",
      "Table contrib             :  786 rows read\n",
      "Table country             :   47 rows read\n",
      "Table help                :    1 rows read\n",
      "Table remark              :  187 rows read\n",
      "Table vcchead             :   14 rows read\n",
      "Table workinggroup        :   20 rows read\n",
      "Data import OK\n",
      "=================================TRANSFORM ROWS=================================\n",
      "Table `contrib`:   786 rows checked\n",
      "Table `country`:    47 rows checked\n",
      "Table `help`:     1 rows checked\n",
      "Table `remark`:   187 rows checked\n",
      "Table `vcchead`:    14 rows checked\n",
      "Table `workinggroup`:    20 rows checked\n",
      "NB: contrib:costs_total \"10000\" <= 10.000\n",
      "NB: contrib:costs_total \"1000000\" <= 1.000.000 \n",
      "\n",
      "NB: contrib:costs_total \"120000\" <= 120.000\n",
      "NB: contrib:costs_total \"12200\" <= 12.200\n",
      "NB: contrib:costs_total \"1281.12\" <= 1281,12\n",
      "NB: contrib:costs_total \"12811.2\" <= 12811,2\n",
      "NB: contrib:costs_total \"143033.12\" <= 143.033,12 â‚¬\n",
      "NB: contrib:costs_total \"158750\" <= 158.750\n",
      "NB: contrib:costs_total \"17550\" <= 17.550\n",
      "NB: contrib:costs_total \"18350\" <= 18.350\n",
      "NB: contrib:costs_total \"20000\" <= â‚¬ 20.000\n",
      "\n",
      "NB: contrib:costs_total \"214500.00\" <= 214.500,00\n",
      "NB: contrib:costs_total \"22800\" <= 22.800\n",
      "NB: contrib:costs_total \"24900\" <= 24.900\n",
      "NB: contrib:costs_total \"252400\" <= 252.400\n",
      "NB: contrib:costs_total \"2650\" <= 2.650\n",
      "NB: contrib:costs_total \"2797.00\" <= 2.797,00\n",
      "NB: contrib:costs_total \"29675\" <= 29.675\n",
      "NB: contrib:costs_total \"3120\" <= 3.120\n",
      "NB: contrib:costs_total \"3375\" <= 3.375\n",
      "NB: contrib:costs_total \"3437.75\" <= 3.437,75\n",
      "NB: contrib:costs_total \"347040.53\" <= 347040,53\n",
      "NB: contrib:costs_total \"36075\" <= 36.075\n",
      "NB: contrib:costs_total \"36828.35\" <= 36.828,35\n",
      "NB: contrib:costs_total \"3900\" <= 3.900\n",
      "NB: contrib:costs_total \"43600\" <= 43.600\n",
      "NB: contrib:costs_total \"4992.00\" <= 4.992,00\n",
      "NB: contrib:costs_total \"5400\" <= 5.400\n",
      "NB: contrib:costs_total \"56652.00\" <= 56.652,00 \n",
      "NB: contrib:costs_total \"60000\" <= 60,000\n",
      "NB: contrib:costs_total \"60170.00\" <= 60.170,00 â‚¬\n",
      "NB: contrib:costs_total \"6125\" <= 6.125 â‚¬\n",
      "NB: contrib:costs_total \"6480\" <= 6.480\n",
      "NB: contrib:costs_total \"6720\" <= 6.720\n",
      "NB: contrib:costs_total \"6789\" <= 6.789\n",
      "NB: contrib:costs_total \"68612.6\" <= 68.612,6\n",
      "NB: contrib:costs_total \"69300\" <= 69.300\n",
      "NB: contrib:costs_total \"6980\" <= 6.980\n",
      "NB: contrib:costs_total \"72064\" <= 72.064\n",
      "NB: contrib:costs_total \"73271.84\" <= 73.271,84\n",
      "NB: contrib:costs_total \"74797.60\" <= 74.797,60\n",
      "NB: contrib:costs_total \"80340.00\" <= 80.340,00\n",
      "NB: contrib:costs_total \"9000\" <= 9.000 Euro\n",
      "NB: contrib:costs_total \"95959.20\" <= 95.959,20\n",
      "NB: contrib:costs_total \"NULL\" <= t.b.c.\n",
      "Money OK\n",
      "===================================PIVOT DATA===================================\n",
      "Table `contrib`: 786   records and 52 fields pivoted\n",
      "Table `country`: 47    records and 5  fields pivoted\n",
      "Table `help`: 1     records and 2  fields pivoted\n",
      "Table `remark`: 187   records and 5  fields pivoted\n",
      "Table `vcchead`: 14    records and 2  fields pivoted\n",
      "Table `workinggroup`: 20    records and 27 fields pivoted\n",
      "Pivot OK\n",
      "==================================MOVE FIELDS===================================\n",
      "moved fields\n",
      "\t`approved`\n",
      "\t`dateandtime_approval`\n",
      "\t`dateandtime_cioapproval`\n",
      "\t`dateandtime_ciozero`\n",
      "\t`submit`\n",
      "\t`vcc11_name`\n",
      "\t`vcc12_name`\n",
      "\t`vcc21_name`\n",
      "\t`vcc22_name`\n",
      "\t`vcc31_name`\n",
      "\t`vcc32_name`\n",
      "\t`vcc41_name`\n",
      "\t`vcc42_name`\n",
      "\t`vcc_head_decision`\n",
      "\t`vcc_head_decision_vcc11`\n",
      "\t`vcc_head_decision_vcc12`\n",
      "\t`vcc_head_decision_vcc21`\n",
      "\t`vcc_head_decision_vcc22`\n",
      "\t`vcc_head_decision_vcc31`\n",
      "\t`vcc_head_decision_vcc32`\n",
      "\t`vcc_head_decision_vcc41`\n",
      "\t`vcc_head_decision_vcc42`\n",
      "\t`vcchead_approval`\n",
      "\t`vcchead_disapproval`\n",
      "from `contrib` to `assessment`\n",
      "Move fields OK\n",
      "==================================REMODEL DATA==================================\n",
      "NB: table `contrib`: value field `country` will be linked to `country:countrycode\n",
      "Extraction OK\n",
      "================================FIX LINKED DATA=================================\n",
      "NB: linking `contrib:country` to table `country` on `countrycode`\n",
      "Fixing OK\n",
      "===================================WRITE SQL====================================\n",
      "NB: Field `description_of_contribution`: value with length 12856 gets type TEXT\n",
      "SQL OK\n",
      "===============================PREPARE MONGO DOCS===============================\n",
      "NB: Field `description_of_contribution`: value with length 12856 gets type TEXT\n",
      "MONGO_TRANSFORM OK\n",
      "================================DEDUP MONGO DOCS================================\n",
      "=================================END PROCESSING=================================\n",
      "OK, no warnings\n"
     ]
    }
   ],
   "source": [
    "info('{:=^80}'.format('BEGIN PROCESSING'))\n",
    "resetw()\n",
    "\n",
    "info('{:=^80}'.format('CHECK CONFIG'))\n",
    "check_config()\n",
    "\n",
    "info('{:=^80}'.format('READ FM'))\n",
    "(root, main_tables_raw) = read_fm()\n",
    "\n",
    "info('{:=^80}'.format('MERGE pre CHECK'))\n",
    "merge_fields = check_merge()\n",
    "\n",
    "info('{:=^80}'.format('FIELD DEFINITIONS'))\n",
    "(tfields, field_defs_raw) = getfielddefs()\n",
    "\n",
    "info('{:=^80}'.format('FIELD VALUE NORMALIZATION'))\n",
    "check_normalize()\n",
    "\n",
    "info('{:=^80}'.format('MERGE post CHECK'))\n",
    "check_merge_more()\n",
    "\n",
    "info('{:=^80}'.format('SKIP FIELDS'))\n",
    "fields_raw = do_skips()\n",
    "\n",
    "info('{:=^80}'.format('READ DATA'))\n",
    "rows_raw = getdata()\n",
    "\n",
    "info('{:=^80}'.format('TRANSFORM ROWS'))\n",
    "rows = transformrows()\n",
    "\n",
    "info('{:=^80}'.format('PIVOT DATA'))\n",
    "field_data_raw = pivot()\n",
    "\n",
    "info('{:=^80}'.format('MOVE FIELDS'))\n",
    "(main_tables, fields, field_defs, field_data) = move_fields()\n",
    "\n",
    "info('{:=^80}'.format('REMODEL DATA'))\n",
    "(maindata, reltables, relxtables, relvalues, relfieldindex) = transform_data()\n",
    "\n",
    "info('{:=^80}'.format('FIX LINKED DATA'))\n",
    "fix_them()\n",
    "\n",
    "info('{:=^80}'.format('WRITE SQL'))\n",
    "sql_export()\n",
    "\n",
    "info('{:=^80}'.format('PREPARE MONGO DOCS'))\n",
    "mongo_db = mongo_transform()\n",
    "\n",
    "info('{:=^80}'.format('DEDUP MONGO DOCS'))\n",
    "mongo_dedup()\n",
    "\n",
    "info('{:=^80}'.format('RENAME MONGO FIELDS'))\n",
    "mongo_rename()\n",
    "\n",
    "info('{:=^80}'.format('END PROCESSING'))\n",
    "finalw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mongo documents are created, now import them into the MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country\n",
      "contrib\n",
      "help\n",
      "remark\n",
      "vcchead\n",
      "workinggroup\n",
      "assessment\n"
     ]
    }
   ],
   "source": [
    "import_mongo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "The process has finished, but here is space to explore the data, in order to find patterns, regularities, and, more importantly, *irregularities*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pprintf(tname, fname):\n",
    "    values_raw = field_data[tname][fname]\n",
    "    values = sorted(v for v in reduce(set.union, values_raw, set()) if v != 'NULL')\n",
    "    print('\\n'.join('{}'.format(v) for v in values))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'AT'\n",
      "'BE'\n",
      "'DE'\n",
      "'DK'\n",
      "'FR'\n",
      "'GR'\n",
      "'HR'\n",
      "'IE'\n",
      "'IT'\n",
      "'LU'\n",
      "'NL'\n",
      "'RS'\n",
      "'SI'\n"
     ]
    }
   ],
   "source": [
    "pprintf('contrib', 'country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table contrib             : 63 fields\n",
      "Table country             :  6 fields\n",
      "Table help                :  2 fields\n",
      "Table remark              :  5 fields\n",
      "Table vcchead             :  2 fields\n",
      "Table workinggroup        : 27 fields\n",
      "Field definitions OK\n",
      "contrib\t\n",
      "\tikid\n",
      "\tikid_base\n",
      "\tcountry\n",
      "\tyear\n",
      "\tvcc\n",
      "\tsubmit\n",
      "\tapproved\n",
      "\ttitle\n",
      "\tacademic_entity_url\n",
      "\tacademic_entity_url_2\n",
      "\tdescription_of_contribution\n",
      "\tcontribution_url\n",
      "\tcontribution_url_2\n",
      "\tcontact_person_mail\n",
      "\tcontact_person_mail_2\n",
      "\tcontact_person_name\n",
      "\tcosts_description\n",
      "\tcosts_total\n",
      "\ttype_of_inkind\n",
      "\tother_type_of_inkind\n",
      "\tdisciplines_associated\n",
      "\ttadirah_research_activities\n",
      "\ttadirah_research_objects\n",
      "\ttadirah_research_techniques\n",
      "\tother_keywords\n",
      "\tcreator\n",
      "\tcreation_date_time\n",
      "\tlast_modifier\n",
      "\tmodification_date_time\n",
      "\ttotal_costs_total\n",
      "\tvcchead_approval\n",
      "\tvcchead_disapproval\n",
      "\twhois\n",
      "\tvcc_head_decision\n",
      "\tdateandtime_approval\n",
      "\tteller\n",
      "\tgoldpassword\n",
      "\tgnewpassword\n",
      "\tgnewpassword2\n",
      "\tmessage\n",
      "\tdateandtime_cioapproval\n",
      "\tdateandtime_ciozero\n",
      "\tvcc_head_decision_vcc11\n",
      "\tvcc_head_decision_vcc12\n",
      "\tvcc_head_decision_vcc21\n",
      "\tvcc_head_decision_vcc22\n",
      "\tvcc_head_decision_vcc31\n",
      "\tvcc_head_decision_vcc32\n",
      "\tvcc_head_decision_vcc41\n",
      "\tvcc_head_decision_vcc42\n",
      "\tvcc11_name\n",
      "\tvcc12_name\n",
      "\tvcc21_name\n",
      "\tvcc22_name\n",
      "\tvcc31_name\n",
      "\tvcc32_name\n",
      "\tvcc41_name\n",
      "\tvcc42_name\n",
      "\thelp_description\n",
      "\thelp_text\n",
      "\tfind_type\n",
      "\tfind_country_id\n",
      "\tmessage_allert\n",
      "workinggroup\t\n",
      "\tworking_group_name\n",
      "\tmain_vcc\n",
      "\tliaison_vcc\n",
      "\tchairs\n",
      "\tparticipant\n",
      "\tpotential_participants\n",
      "\tproponents\n",
      "\tscope\n",
      "\tgeneral_contribution_to_dariah_infrastructure\n",
      "\trelation_to_dariah_objectives\n",
      "\tpartnership\n",
      "\tactivities\n",
      "\tdeliverables\n",
      "\tagenda\n",
      "\tin_kind_provisions\n",
      "\tadditional_funding\n",
      "\tfuture_location_of_the_service\n",
      "\tdate_proposal\n",
      "\tdate_approval_jrc\n",
      "\tdate_approval_smt\n",
      "\textra_info\n",
      "\ttheme__access\n",
      "\ttheme__sustainability\n",
      "\ttheme__impact\n",
      "\ttheme__interoperability\n",
      "\ttheme__training\n",
      "\tnumber\n",
      "help\t\n",
      "\thelp_description\n",
      "\thelp_text\n",
      "vcchead\t\n",
      "\tvcccode\n",
      "\tvcc_head_name\n",
      "remark\t\n",
      "\tikid\n",
      "\tremarker\n",
      "\tremarks\n",
      "\tremark_date_time\n",
      "\tremarkid\n",
      "country\t\n",
      "\tcountrycode\n",
      "\tcountryname\n",
      "\tmember_dariah\n",
      "\tname\n",
      "\tlatitude\n",
      "\tlongitude\n"
     ]
    }
   ],
   "source": [
    "fdefs = getfielddefs()[0]\n",
    "for tb in fdefs:\n",
    "    print('{}\\t{}'.format(tb, ''))\n",
    "    for fld in fdefs[tb]:\n",
    "        print('{}\\t{}'.format('', fld))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MongoDB content\n",
    "How does the data end up in MongoDB?\n",
    "Let us show a few contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "_id=0\n",
      "academic_entity_url=[{'value': 'http://www.bilingualism-matters-rijeka.ffri.hr/', '_id': '1'}]\n",
      "contact_person_mail=[{'value': 'https://portal.uniri.hr/portfelj/tkras@uniri.hr', '_id': '1'}]\n",
      "contact_person_name=Tihana KraÅ¡\n",
      "contribution_url=[{'value': 'http://www.bilingualism-matters-rijeka.ffri.hr/en/', '_id': '1'}]\n",
      "costs_description=â€¢\\tOperating costs\\n\\to\\tTravels: \\n\\to\\tService provisions: 3.000,00 â‚¬\\n\\to\\tOther costs: 1.000,00 â‚¬\\n\\nTotal: 4.000,00 â‚¬\n",
      "costs_total=4000.0\n",
      "country=[{'name': 'Croatia', '_id': 'HR'}]\n",
      "creation_date_time=None\n",
      "creator={'value': None, '_id': '1'}\n",
      "description_of_contribution=The partner will contribute with its website, which contains information about bilingualism intended for general public. It will also offer expert advice on bilingualism by its team members, provided by e-mail and in person (if appropriate).\n",
      "disciplines_associated=[{'value': 'Linguistics', '_id': '1'}, {'value': 'Communication sciences', '_id': '2'}, {'value': 'Psychology', '_id': '3'}, {'value': 'Education', '_id': '4'}]\n",
      "find_country_id=None\n",
      "find_type=None\n",
      "ikid=201500001\n",
      "ikid_base=None\n",
      "last_modifier={'value': 'ITNC01', '_id': '1'}\n",
      "message=None\n",
      "message_allert=None\n",
      "modification_date_time=2016-06-20 16:02:44\n",
      "other_keywords=[{'value': 'dissemination', '_id': '1'}, {'value': 'advice', '_id': '2'}, {'value': 'bilingualism', '_id': '3'}, {'value': 'benefits', '_id': '4'}, {'value': 'information', '_id': '5'}]\n",
      "other_type_of_inkind={'value': None, '_id': '1'}\n",
      "tadirah_research_activities=[{'value': '7. Dissemination', '_id': '1'}]\n",
      "tadirah_research_objects=[{'value': 'Research Results', '_id': '1'}]\n",
      "tadirah_research_techniques=[{'value': 'Linked open data > Enrichment-Annotation; Dissemination-Publishing', '_id': '1'}]\n",
      "title=Bilingualism Matters@Rijeka\n",
      "type_of_inkind=[{'value': 'Access', '_id': '1'}, {'value': 'Expertise', '_id': '2'}]\n",
      "vcc=[{'value': 'VCC4', '_id': '1'}]\n",
      "year={'value': '2015', '_id': '1'}\n",
      "==================================================\n",
      "_id=1\n",
      "academic_entity_url=[{'value': 'http://www.ffri.hr', '_id': '2'}]\n",
      "contact_person_mail=[{'value': 'silvana.vranic@uniri.hr', '_id': '2'}, {'value': 'mailto:silvana.vranic@uniri.hr', '_id': '3'}, {'value': 'https://portal.uniri.hr/portfelj/silvana.vranic@uniri.hr', '_id': '4'}]\n",
      "contact_person_name=Silvana VraniÄ‡\n",
      "costs_description=â€¢\\tSalaries\\n\\ta.\\tannual percentage given to the contribution\\n\\tb.\\tannual salary (real salary or use an average grid)\\n\\tc.\\toverhead\\n\\td.\\ta Ã— b Ã— c\\n\\nFirst Name Last Name\\ta\\tb\\tc\\td = a Ã— b Ã— c\\t\\nAnastazija VlasteliÄ‡\\t0.30\\t20.000,00\\t1.66\\t9,960,00\\t\\nSaÅ¡a PotoÄnjak\\t0.30\\t20.000,00\\t1.66\\t9,960,00\\t\\nSilvana VraniÄ‡\\t0.40\\t20.000,00\\t1.66\\t13.280,00\\t\\n\\nTotal: 33.200,00 â‚¬\n",
      "costs_total=33200.0\n",
      "country=[{'name': 'Croatia', '_id': 'HR'}]\n",
      "creation_date_time=None\n",
      "creator={'value': None, '_id': '1'}\n",
      "description_of_contribution=I8ik,T2As the main editor of the magazine Fluminensia, my team and I uploaded all the editions of the magazine published between the years 1989 to 1999, as well as edition Nr. 2 of 2014, on the website HrÄak  (http://hrcak.srce.hr/fluminensia) and on the magazineâ€™s own website  (http://fluminensia.ffri.hr). The publication of another two editions is scheduled for 2015.\n",
      "disciplines_associated=[{'value': 'Cultural heritage and museology', '_id': '5'}, {'value': 'Literature', '_id': '6'}, {'value': 'Social Anthropology and ethnology', '_id': '7'}, {'value': 'Management', '_id': '8'}, {'value': 'Linguistics', '_id': '1'}, {'value': 'History, Philosophy and Sociology of Sciences', '_id': '9'}]\n",
      "find_country_id=None\n",
      "find_type=None\n",
      "ikid=201500002\n",
      "ikid_base=None\n",
      "last_modifier={'value': 'VCC42', '_id': '2'}\n",
      "message=None\n",
      "message_allert=None\n",
      "modification_date_time=2016-05-08 15:49:18\n",
      "other_type_of_inkind={'value': None, '_id': '1'}\n",
      "tadirah_research_activities=[{'value': '6. Storage', '_id': '2'}]\n",
      "tadirah_research_objects=[{'value': 'Language', '_id': '2'}, {'value': 'Literature', '_id': '3'}, {'value': 'Named Entities', '_id': '4'}, {'value': 'Data', '_id': '5'}, {'value': 'Manuscript', '_id': '6'}, {'value': 'Bibliographic Listings', '_id': '7'}]\n",
      "tadirah_research_techniques=[{'value': 'Linked open data > Enrichment-Annotation; Dissemination-Publishing', '_id': '1'}, {'value': 'Searching', '_id': '2'}]\n",
      "title=Journal Archives of Fluminensia, Journal for philological research, FFRi, Croatia\n",
      "type_of_inkind=[{'value': 'Educational Resources', '_id': '3'}, {'value': 'DARIAH Coordination', '_id': '4'}, {'value': 'Interoperability', '_id': '5'}, {'value': 'Data', '_id': '6'}, {'value': 'Access', '_id': '1'}, {'value': 'Content Hosting', '_id': '7'}, {'value': 'Cooperation', '_id': '8'}]\n",
      "vcc=[{'value': 'VCC2', '_id': '2'}, {'value': 'VCC3', '_id': '3'}]\n",
      "year={'value': '2015', '_id': '1'}\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient()\n",
    "dbm = client.dariah\n",
    "for d in dbm.contrib.find({}).limit(2):\n",
    "    print('=' * 50)\n",
    "    for f in sorted(d):\n",
    "        print('{}={}'.format(f, d[f]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a query to get all 'type_of_inkind' values for contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Access\n",
      "  1 Content Hosting\n",
      "  1 Cooperation\n",
      "  1 DARIAH Coordination\n",
      "  1 Data\n",
      "  1 Educational Resources\n",
      "  1 Event\n",
      "  1 Expertise\n",
      "  1 Interoperability\n",
      "  1 Summer School\n",
      "  1 Tools and Software\n",
      "  1 Training\n",
      "  1 1\n",
      "  1 10\n",
      "  1 11\n",
      "  1 12\n",
      "  1 2\n",
      "  1 3\n",
      "  1 4\n",
      "  1 5\n",
      "  1 6\n",
      "  1 7\n",
      "  1 8\n",
      "  1 9\n"
     ]
    }
   ],
   "source": [
    "cnt_type = collections.Counter()\n",
    "cnt_type_id = collections.Counter()\n",
    "for c in dbm.contrib.distinct('type_of_inkind', {}):\n",
    "    cnt_type[c['value']] += 1\n",
    "    cnt_type_id[c['_id']] += 1\n",
    "for (t,c) in sorted(cnt_type.items(), key=lambda x: (-x[1], x[0])):\n",
    "    print('{:>3} {}'.format(c, t))\n",
    "for (t,c) in sorted(cnt_type_id.items(), key=lambda x: (-x[1], x[0])):\n",
    "    print('{:>3} {}'.format(c, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Austria', 'member_dariah': True, 'latitude': 47.7, 'longitude': 15.11, '_id': 'AT'}\n",
      "{'name': 'Belgium', 'member_dariah': True, 'latitude': 51.3, 'longitude': 3.1, '_id': 'BE'}\n",
      "{'name': 'Croatia', 'member_dariah': True, 'latitude': 44.7, 'longitude': 15.6, '_id': 'HR'}\n",
      "{'name': 'Cyprus', 'member_dariah': True, 'latitude': 35.0, 'longitude': 32.8, '_id': 'CY'}\n",
      "{'name': 'Czech Republic', 'member_dariah': False, 'latitude': 49.8, 'longitude': 15.2, '_id': 'CZ'}\n",
      "{'name': 'Denmark', 'member_dariah': True, 'latitude': 55.6, 'longitude': 11.0, '_id': 'DK'}\n",
      "{'name': 'Estonia', 'member_dariah': False, 'latitude': 59.0, 'longitude': 25.0, '_id': 'EE'}\n",
      "{'name': 'France', 'member_dariah': True, 'latitude': 46.5, 'longitude': 1.9, '_id': 'FR'}\n",
      "{'name': 'Germany', 'member_dariah': True, 'latitude': 51.0, 'longitude': 10.4, '_id': 'DE'}\n",
      "{'name': 'Greece', 'member_dariah': True, 'latitude': 38.0, 'longitude': 23.8, '_id': 'GR'}\n",
      "{'name': 'Hungary', 'member_dariah': False, 'latitude': 46.9, 'longitude': 19.8, '_id': 'HU'}\n",
      "{'name': 'Ireland', 'member_dariah': True, 'latitude': 53.1, 'longitude': -8.4, '_id': 'IE'}\n",
      "{'name': 'Italy', 'member_dariah': True, 'latitude': 41.6, 'longitude': 13.0, '_id': 'IT'}\n",
      "{'name': 'Latvia', 'member_dariah': False, 'latitude': 56.9, 'longitude': 26.8, '_id': 'LV'}\n",
      "{'name': 'Lithuania', 'member_dariah': False, 'latitude': 55.2, 'longitude': 24.9, '_id': 'LT'}\n",
      "{'name': 'Luxembourg', 'member_dariah': True, 'latitude': 49.6, 'longitude': 6.1, '_id': 'LU'}\n",
      "{'name': 'Malta', 'member_dariah': True, 'latitude': 35.9, 'longitude': 14.4, '_id': 'MT'}\n",
      "{'name': 'Netherlands', 'member_dariah': True, 'latitude': 52.8, 'longitude': 5.8, '_id': 'NL'}\n",
      "{'name': 'Portugal', 'member_dariah': True, 'latitude': 38.7, 'longitude': -9.0, '_id': 'PT'}\n",
      "{'name': 'Serbia', 'member_dariah': True, 'latitude': 44.0, 'longitude': 20.8, '_id': 'RS'}\n",
      "{'name': 'Slovakia', 'member_dariah': False, 'latitude': 48.8, 'longitude': 19.9, '_id': 'SK'}\n",
      "{'name': 'Switzerland', 'member_dariah': False, 'latitude': 46.9, 'longitude': 8.3, '_id': 'CH'}\n",
      "{'name': 'United Kingdom', 'member_dariah': False, 'latitude': 52.9, 'longitude': -1.8, '_id': 'GB'}\n",
      "{'name': 'Slovenia', 'member_dariah': True, 'latitude': 46.2, 'longitude': 14.4, '_id': 'SI'}\n",
      "{'name': 'Poland', 'member_dariah': True, 'latitude': 52.3, 'longitude': 19.8, '_id': 'PL'}\n",
      "{'name': 'Spain', 'member_dariah': False, 'latitude': 39.8, 'longitude': -3.4, '_id': 'ES'}\n",
      "{'name': 'Macedonia', 'member_dariah': False, 'latitude': 41.6, 'longitude': 21.8, '_id': 'MK'}\n",
      "{'name': 'Andorra', 'member_dariah': False, 'latitude': 42.5, 'longitude': 1.6, '_id': 'AD'}\n",
      "{'name': 'Romania', 'member_dariah': False, 'latitude': 45.8, 'longitude': 24.8, '_id': 'RO'}\n",
      "{'name': 'Russian Federation', 'member_dariah': False, 'latitude': 55.6, 'longitude': 37.7, '_id': 'RU'}\n",
      "{'name': 'Belarus', 'member_dariah': False, 'latitude': 53.8, 'longitude': 29.2, '_id': 'BY'}\n",
      "{'name': 'Finland', 'member_dariah': False, 'latitude': 63.3, 'longitude': 27.6, '_id': 'FI'}\n",
      "{'name': 'Moldova', 'member_dariah': False, 'latitude': 47.3, 'longitude': 28.7, '_id': 'MD'}\n",
      "{'name': 'Sweden', 'member_dariah': False, 'latitude': 59.5, 'longitude': 16.1, '_id': 'SE'}\n",
      "{'name': 'Albania', 'member_dariah': False, 'latitude': 41.1, 'longitude': 19.9, '_id': 'AL'}\n",
      "{'name': 'Georgia', 'member_dariah': False, 'latitude': 41.66, 'longitude': 43.68, '_id': 'GE'}\n",
      "{'name': 'Monaco', 'member_dariah': False, 'latitude': 43.7, 'longitude': 7.4, '_id': 'MC'}\n",
      "{'name': 'Kosovo', 'member_dariah': False, 'latitude': 43.2, 'longitude': 21.9, '_id': 'KS'}\n",
      "{'name': 'Montenegro', 'member_dariah': False, 'latitude': 42.3, 'longitude': 19.2, '_id': 'ME'}\n",
      "{'name': 'Turkey', 'member_dariah': False, 'latitude': 40.0, 'longitude': 32.8, '_id': 'TR'}\n",
      "{'name': 'Bulgaria', 'member_dariah': False, 'latitude': 42.9, 'longitude': 26.5, '_id': 'BG'}\n",
      "{'name': 'Liechtenstein', 'member_dariah': False, 'latitude': 47.2, 'longitude': 9.4, '_id': 'LI'}\n",
      "{'name': 'San Marino', 'member_dariah': False, 'latitude': 43.8, 'longitude': 12.3, '_id': 'SM'}\n",
      "{'name': 'Bosnia and Herzegovina', 'member_dariah': False, 'latitude': 44.2, 'longitude': 18.2, '_id': 'BA'}\n",
      "{'name': 'Ukraine', 'member_dariah': False, 'latitude': 49.3, 'longitude': 32.6, '_id': 'UA'}\n",
      "{'name': 'Norway', 'member_dariah': False, 'latitude': 62.0, 'longitude': 7.1, '_id': 'NO'}\n",
      "{'name': 'Iceland', 'member_dariah': False, 'latitude': 65.0, 'longitude': -18.8, '_id': 'IS'}\n"
     ]
    }
   ],
   "source": [
    "for c in dbm.country.find({}):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get related data: the type_of_inkind of all contributions. For each contribution we need only the ids of the related type_of_inkind values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type_of_inkind': [{'_id': '1'}, {'_id': '2'}], '_id': '0'}\n",
      "{'type_of_inkind': [{'_id': '3'}, {'_id': '4'}, {'_id': '5'}, {'_id': '6'}, {'_id': '1'}, {'_id': '7'}, {'_id': '8'}], '_id': '1'}\n",
      "{'type_of_inkind': [{'_id': '9'}, {'_id': '10'}], '_id': '2'}\n",
      "{'type_of_inkind': [{'_id': '4'}], '_id': '3'}\n",
      "{'type_of_inkind': [{'_id': '8'}, {'_id': '4'}], '_id': '4'}\n",
      "{'type_of_inkind': [{'_id': '1'}], '_id': '5'}\n",
      "{'type_of_inkind': [{'_id': '10'}], '_id': '6'}\n",
      "{'type_of_inkind': [{'_id': '8'}], '_id': '7'}\n",
      "{'type_of_inkind': [{'_id': '1'}], '_id': '8'}\n",
      "{'type_of_inkind': [{'_id': '4'}], '_id': '9'}\n"
     ]
    }
   ],
   "source": [
    "for d in dbm.contrib.find({}, {'type_of_inkind._id': True}).limit(10):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country': [{'_id': 'HR'}], '_id': '0'}\n",
      "{'country': [{'_id': 'HR'}], '_id': '1'}\n",
      "{'country': [{'_id': 'HR'}], '_id': '2'}\n",
      "{'country': [{'_id': 'HR'}], '_id': '3'}\n",
      "{'country': [{'_id': 'HR'}], '_id': '4'}\n",
      "{'country': [{'_id': 'HR'}], '_id': '5'}\n",
      "{'country': [{'_id': 'HR'}], '_id': '6'}\n",
      "{'country': [{'_id': 'HR'}], '_id': '7'}\n",
      "{'country': [{'_id': 'HR'}], '_id': '8'}\n",
      "{'country': [{'_id': 'LU'}], '_id': '9'}\n"
     ]
    }
   ],
   "source": [
    "for d in dbm.contrib.find({}, {'country._id': True}).limit(10):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
